{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AnkurMali/IST597_Spring_2022/blob/main/IST597_MLP_collab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "71kdFp0QgF4K"
      },
      "source": [
        "# IST597:- Multi-Layer Perceptron"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C2yHcl5xgPV1"
      },
      "source": [
        "## Load the libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2DPwxLR2gSLC"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import time\n",
        "import tensorflow as tf\n",
        "np.random.seed(1234)\n",
        "tf.random.set_seed(1234)\n",
        "from tensorflow.keras.datasets import mnist\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import LabelBinarizer\n",
        "from sklearn.metrics import roc_auc_score, accuracy_score\n",
        "from sklearn.metrics import precision_score, accuracy_score\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wV-3kEaggcO8",
        "outputId": "67d3cb8d-766b-4589-8f78-71b4ea77e769"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[]"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "tf.config.list_physical_devices('GPU')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iw78jw6pDqSM"
      },
      "source": [
        "#Get number of Gpu's and id's in the system or else you can also use Nvidia-smi in command prompt."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Dk_S2TMg_6_"
      },
      "source": [
        "## Generate random data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "40XlFnwho7D8"
      },
      "outputs": [],
      "source": [
        "size_input = 784\n",
        "size_hidden1 = 256\n",
        "size_hidden2 = 128\n",
        "size_output = 10\n",
        "\n",
        "number_of_train_examples = 6000\n",
        "number_of_test_examples = 1000"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 551
        },
        "id": "qm23CzRihaW0",
        "outputId": "d65c147c-2095-4352-f9fc-f892c849decd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11493376/11490434 [==============================] - 0s 0us/step\n",
            "11501568/11490434 [==============================] - 0s 0us/step\n",
            "(50000, 28, 28) (50000,)\n",
            "Train dimension:\n",
            "(50000, 784)\n",
            "Test dimension:\n",
            "(10000, 784)\n",
            "Val dimension:\n",
            "(10000, 784)\n",
            "Train labels dimension:\n",
            "(50000, 10)\n",
            "Test labels dimension:\n",
            "(10000, 10)\n",
            "Test labels dimension:\n",
            "(10000, 10)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "'''\n",
        "X_train = np.random.randn(number_of_train_examples , size_input)\n",
        "y_train = np.random.randn(number_of_train_examples)\n",
        "X_test = np.random.randn(number_of_test_examples, size_input)\n",
        "y_test = np.random.randn(number_of_test_examples)\n",
        "'''\n",
        "\n",
        "\n",
        "def load_dataset(flatten=False):\n",
        "    (X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
        "    # normalize x\n",
        "    X_train = X_train.astype(float)/255.0\n",
        "    X_test = X_test.astype(float)/255.0\n",
        "    # we reserve the last 10000 training examples for validation\n",
        "    X_train, X_val = X_train[:-10000], X_train[-10000:]\n",
        "    y_train, y_val = y_train[:-10000], y_train[-10000:]\n",
        "    if flatten:\n",
        "        X_train = X_train.reshape([X_train.shape[0], -1])\n",
        "        X_val = X_val.reshape([X_val.shape[0], -1])\n",
        "        X_test = X_test.reshape([X_test.shape[0], -1])\n",
        "    return X_train, y_train, X_val, y_val, X_test, y_test\n",
        "\n",
        "X_train, y_train, X_val, y_val, X_test, y_test = load_dataset()\n",
        "## Printing dimensions\n",
        "print(X_train.shape, y_train.shape)\n",
        "## Visualizing the first digit\n",
        "plt.imshow(X_train[0], cmap=\"Greys\");\n",
        "## Changing dimension of input images from N*28*28 to  N*784\n",
        "X_train = X_train.reshape((X_train.shape[0],X_train.shape[1]*X_train.shape[2]))\n",
        "X_test = X_test.reshape((X_test.shape[0],X_test.shape[1]*X_test.shape[2]))\n",
        "X_val = X_val.reshape((X_val.shape[0],X_val.shape[1]*X_val.shape[2]))\n",
        "\n",
        "print('Train dimension:');print(X_train.shape)\n",
        "print('Test dimension:');print(X_test.shape)\n",
        "print('Val dimension:');print(X_val.shape)\n",
        "\n",
        "## Changing labels to one-hot encoded vector\n",
        "lb = LabelBinarizer()\n",
        "y_train = lb.fit_transform(y_train)\n",
        "y_test = lb.transform(y_test)\n",
        "y_val = lb.transform(y_val)\n",
        "\n",
        "print('Train labels dimension:');print(y_train.shape)\n",
        "print('Test labels dimension:');print(y_test.shape)\n",
        "print('Test labels dimension:');print(y_val.shape)\n",
        "\n",
        "\n",
        "def Accuracy(yTrue,yPred):\n",
        "    return tf.keras.metrics.sparse_categorical_accuracy(yTrue, yPred)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aigqKFFF5BM2"
      },
      "outputs": [],
      "source": [
        "# Split dataset into batches\n",
        "train_ds = tf.data.Dataset.from_tensor_slices((X_train, y_train)).batch(128)\n",
        "test_ds = tf.data.Dataset.from_tensor_slices((X_test, y_test)).batch(128)\n",
        "\n",
        "\n",
        "   "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Eb4hOoVbnzSJ"
      },
      "source": [
        "## Build MLP using Eager Execution"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ht9_qpYipgHw"
      },
      "outputs": [],
      "source": [
        "# Define class to build mlp model\n",
        "class MLP(object):\n",
        "  def __init__(self, size_input, size_hidden1,size_hidden2,size_output, device=None):\n",
        "    \"\"\"\n",
        "    size_input: int, size of input layer\n",
        "    size_hidden: int, size of hidden layer\n",
        "    size_output: int, size of output layer\n",
        "    device: str or None, either 'cpu' or 'gpu' or None. If None, the device to be used will be decided automatically during Eager Execution\n",
        "    \"\"\"\n",
        "    self.size_input, self.size_hidden1,self.size_hidden2, self.size_output, self.device =\\\n",
        "    size_input, size_hidden1, size_hidden2, size_output, device\n",
        "    \n",
        "    # Initialize weights between input layer1 and hidden layer1\n",
        "    self.W1 = tf.Variable(tf.random.normal([self.size_input, self.size_hidden1]))\n",
        "    # Initialize biases for hidden layer1\n",
        "    self.b1 = tf.Variable(tf.random.normal([1, self.size_hidden1]))\n",
        "     # Initialize weights between hidden layer1 and hidden layer2 layer\n",
        "    self.W2 = tf.Variable(tf.random.normal([self.size_hidden1, self.size_hidden2]))\n",
        "    # Initialize biases for hidden hidden layer2 layer\n",
        "    self.b2 = tf.Variable(tf.random.normal([1, self.size_hidden2]))\n",
        "    # Initialize weights between hidden layer2 and output layer\n",
        "    self.W3 = tf.Variable(tf.random.normal([self.size_hidden2, self.size_output]))\n",
        "    # Initialize biases for output layer\n",
        "    self.b3 = tf.Variable(tf.random.normal([1, self.size_output]))\n",
        "    \n",
        "    # Define variables to be updated during backpropagation\n",
        "    self.variables = [self.W1, self.W2, self.b1, self.b2,self.W3,self.b3]\n",
        "    \n",
        "  def forward(self, X):\n",
        "    \"\"\"\n",
        "    forward pass\n",
        "    X: Tensor, inputs\n",
        "    \"\"\"\n",
        "    if self.device is not None:\n",
        "      with tf.device('gpu:0' if self.device=='gpu' else 'cpu'):\n",
        "        self.y = self.compute_output(X)\n",
        "    else:\n",
        "      self.y = self.compute_output(X)\n",
        "      \n",
        "    return self.y\n",
        "  \n",
        "  def loss(self, y_pred, y_true):\n",
        "    '''\n",
        "    y_pred - Tensor of shape (batch_size, size_output)\n",
        "    y_true - Tensor of shape (batch_size, size_output)\n",
        "    '''\n",
        "    y_true_tf = tf.cast(tf.reshape(y_true, (-1, self.size_output)), dtype=tf.float32)\n",
        "    y_pred_tf = tf.cast(y_pred, dtype=tf.float32)\n",
        "    cce = tf.keras.losses.CategoricalCrossentropy()\n",
        "    #print(y_true_tf.shape,y_pred_tf.shape)\n",
        "    #print('succ',cce(y_true_tf, y_pred_tf).numpy())\n",
        "    loss = cce(y_true_tf, y_pred_tf)\n",
        "    beta = 0.01\n",
        "    # Loss function with L2 Regularization with beta=0.01\n",
        "    regularizers = tf.nn.l2_loss(self.W1) + tf.nn.l2_loss(self.W2) + tf.nn.l2_loss(self.W3)\n",
        "    loss = tf.reduce_mean(loss + beta * regularizers)\n",
        "    return loss\n",
        "    #return tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=y_pred_tf, labels=y_true_tf))\n",
        "    #return tf.losses.mean_squared_error(y_true_tf, y_pred_tf)\n",
        "  \n",
        "  def backward(self, X_train, y_train):\n",
        "    \"\"\"\n",
        "    backward pass\n",
        "    \"\"\"    \n",
        "    \n",
        "    optimizer = tf.keras.optimizers.SGD(learning_rate=1e-2)\n",
        "    \n",
        "    with tf.GradientTape() as tape:\n",
        "      predicted = self.forward(X_train)\n",
        "      current_loss = self.loss(predicted, y_train)\n",
        "    grads = tape.gradient(current_loss, self.variables)\n",
        "    optimizer.apply_gradients(zip(grads, self.variables))\n",
        "    \n",
        "    '''\n",
        "\n",
        "    with tf.GradientTape(persistent=True) as tape3:\n",
        "      with tf.GradientTape(persistent=True) as tape2:\n",
        "        with tf.GradientTape(persistent=True) as tape1:\n",
        "          predicted = self.forward(X_train)\n",
        "          current_loss = self.loss(predicted, y_train)\n",
        "        grad1 = tape1.gradient(current_loss, [self.W3,self.b3])\n",
        "        optimizer.apply_gradients(zip(grad1, [self.W3,self.b3]))\n",
        "      grad2 = tape2.gradient(grad1,[self.W2,self.b2])\n",
        "      optimizer.apply_gradients(zip(grad2, [self.W2,self.b2]))\n",
        "    grad3 = tape3.gradient(grad2, [self.W1,self.b1])\n",
        "    optimizer.apply_gradients(zip(grad3, [self.W1,self.b1]))\n",
        "    return grad3\n",
        "\n",
        "    '''\n",
        "        \n",
        "  def compute_output(self, X):\n",
        "    \"\"\"\n",
        "    Custom method to obtain output tensor during forward pass\n",
        "    \"\"\"\n",
        "    # Cast X to float32\n",
        "    X_tf = tf.cast(X, dtype=tf.float32)\n",
        "    #Remember to normalize your dataset before moving forward\n",
        "    # Compute values in hidden layer1\n",
        "    what = tf.matmul(X_tf, self.W1) + self.b1\n",
        "    hhat = tf.nn.relu(what)\n",
        "    # Compute values in hidden layer2\n",
        "    what2 = tf.matmul(hhat, self.W2) + self.b2\n",
        "    hhat2 = tf.nn.relu(what2)\n",
        "    # Compute output\n",
        "    output = tf.matmul(hhat2, self.W3) + self.b3\n",
        "    #Now consider two things , First look at inbuild loss functions if they work with softmax or not and then change this\n",
        "    #Second add tf.Softmax(output) and then return this variable\n",
        "    return tf.nn.softmax(output)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cUDFOuNk618X"
      },
      "source": [
        "## Train Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FZPVUu0YDa-_"
      },
      "outputs": [],
      "source": [
        "# Set number of epochs\n",
        "NUM_EPOCHS = 30"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "xI4lsqPhB6Xi",
        "outputId": "c115dca5-0399-46f1-ce49-0e26fda1f260"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "For Seed 56 :\n",
            "Number of Epoch = 5 - Average Loss:= 9.234468125 Validation Accuracy:= 51.910001039505005 Training Accuracy:= 52.046000957489014\n",
            "Number of Epoch = 10 - Average Loss:= 4.26374125 Validation Accuracy:= 63.13999891281128 Training Accuracy:= 63.55000138282776\n",
            "Number of Epoch = 15 - Average Loss:= 1.96948875 Validation Accuracy:= 78.64000201225281 Training Accuracy:= 79.24200296401978\n",
            "Number of Epoch = 20 - Average Loss:= 0.87288078125 Validation Accuracy:= 93.58999729156494 Training Accuracy:= 94.40400004386902\n",
            "Number of Epoch = 25 - Average Loss:= 0.4020761328125 Validation Accuracy:= 94.74999904632568 Training Accuracy:= 95.2180027961731\n",
            "Number of Epoch = 30 - Average Loss:= 0.18796169921875 Validation Accuracy:= 94.74999904632568 Training Accuracy:= 94.85399723052979\n",
            "\n",
            "Total time taken (in seconds): 614.80\n",
            "Inference Loss: 0.1750\n",
            "Inference Accuracy: 94.7452\n",
            "For Seed 876 :\n",
            "Number of Epoch = 5 - Average Loss:= 9.316155 Validation Accuracy:= 37.72000074386597 Training Accuracy:= 38.74000012874603\n",
            "Number of Epoch = 10 - Average Loss:= 4.342775 Validation Accuracy:= 38.53000104427338 Training Accuracy:= 39.67199921607971\n",
            "Number of Epoch = 15 - Average Loss:= 1.995294375 Validation Accuracy:= 76.13999843597412 Training Accuracy:= 75.77400207519531\n",
            "Number of Epoch = 20 - Average Loss:= 0.87821953125 Validation Accuracy:= 93.4000015258789 Training Accuracy:= 93.48400235176086\n",
            "Number of Epoch = 25 - Average Loss:= 0.404486328125 Validation Accuracy:= 94.91000175476074 Training Accuracy:= 94.68799829483032\n",
            "Number of Epoch = 30 - Average Loss:= 0.18910330078125 Validation Accuracy:= 95.10999917984009 Training Accuracy:= 94.63000297546387\n",
            "\n",
            "Total time taken (in seconds): 575.16\n",
            "Inference Loss: 0.1759\n",
            "Inference Accuracy: 94.5661\n",
            "For Seed 6666 :\n",
            "Number of Epoch = 5 - Average Loss:= 9.290006875 Validation Accuracy:= 46.75999879837036 Training Accuracy:= 46.922001242637634\n",
            "Number of Epoch = 10 - Average Loss:= 4.2780834375 Validation Accuracy:= 64.53999876976013 Training Accuracy:= 64.27800059318542\n",
            "Number of Epoch = 15 - Average Loss:= 1.9727375 Validation Accuracy:= 73.54000210762024 Training Accuracy:= 73.84799718856812\n",
            "Number of Epoch = 20 - Average Loss:= 0.878593671875 Validation Accuracy:= 93.4000015258789 Training Accuracy:= 93.55199933052063\n",
            "Number of Epoch = 25 - Average Loss:= 0.40431609375 Validation Accuracy:= 94.96999979019165 Training Accuracy:= 94.97600197792053\n",
            "Number of Epoch = 30 - Average Loss:= 0.18902837890625 Validation Accuracy:= 95.23000121116638 Training Accuracy:= 94.91599798202515\n",
            "\n",
            "Total time taken (in seconds): 562.24\n",
            "Inference Loss: 0.1759\n",
            "Inference Accuracy: 94.6457\n",
            "For Seed 7777 :\n",
            "Number of Epoch = 5 - Average Loss:= 9.233200625 Validation Accuracy:= 47.15000092983246 Training Accuracy:= 46.639999747276306\n",
            "Number of Epoch = 10 - Average Loss:= 4.2892746875 Validation Accuracy:= 49.36999976634979 Training Accuracy:= 49.28399920463562\n",
            "Number of Epoch = 15 - Average Loss:= 1.96106171875 Validation Accuracy:= 75.41000247001648 Training Accuracy:= 75.00200271606445\n",
            "Number of Epoch = 20 - Average Loss:= 0.87180125 Validation Accuracy:= 93.07000041007996 Training Accuracy:= 93.57799887657166\n",
            "Number of Epoch = 25 - Average Loss:= 0.401830390625 Validation Accuracy:= 94.60999965667725 Training Accuracy:= 94.60200071334839\n",
            "Number of Epoch = 30 - Average Loss:= 0.1879849609375 Validation Accuracy:= 94.9999988079071 Training Accuracy:= 94.61399912834167\n",
            "\n",
            "Total time taken (in seconds): 574.19\n",
            "Inference Loss: 0.1749\n",
            "Inference Accuracy: 94.3372\n",
            "For Seed 3452 :\n",
            "Number of Epoch = 5 - Average Loss:= 9.202939375 Validation Accuracy:= 71.17999792098999 Training Accuracy:= 70.14999985694885\n",
            "Number of Epoch = 10 - Average Loss:= 4.2386390625 Validation Accuracy:= 74.47999715805054 Training Accuracy:= 74.617999792099\n",
            "Number of Epoch = 15 - Average Loss:= 1.91873609375 Validation Accuracy:= 92.35000014305115 Training Accuracy:= 92.3799991607666\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-4ec58430478d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0mtrain_ds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_tensor_slices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m25\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0maccuracies\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_ds\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m       \u001b[0mpreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmlp_on_default\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m       \u001b[0mloss_total_gpu\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_total_gpu\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mmlp_on_default\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "seed = [56,876,6666,7777,3452,5]\n",
        "\n",
        "for x in seed:\n",
        "  print('For Seed',x,':')\n",
        "\n",
        "  #Default mode\n",
        "  mlp_on_default = MLP(size_input, size_hidden1, size_hidden2, size_output, device='tpu')\n",
        "  #hidden2 = MLP(size_hidden2, size_hidden2, size_output, device='gpu')\n",
        "  time_start = time.time()\n",
        "\n",
        "  for epoch in range(NUM_EPOCHS):\n",
        "    loss_total_gpu = tf.zeros([1,1], dtype=tf.float32)\n",
        "    lt = 0\n",
        "    acc = 0\n",
        "    train_ds = tf.data.Dataset.from_tensor_slices((X_train, y_train)).shuffle(25, seed=epoch*(x)).batch(64)\n",
        "    accuracies = []\n",
        "    for inputs, outputs in train_ds:\n",
        "      preds = mlp_on_default.forward(inputs)\n",
        "      loss_total_gpu = loss_total_gpu + mlp_on_default.loss(preds, outputs)\n",
        "      acc = loss_total_gpu + mlp_on_default.loss(preds, outputs)\n",
        "      lt = lt + mlp_on_default.loss(preds, outputs)\n",
        "      mlp_on_default.backward(inputs, outputs)\n",
        "      \n",
        "    Accpred = mlp_on_default.forward(X_train)\n",
        "    cat_acc = tf.metrics.CategoricalAccuracy()\n",
        "    accuracies.append(cat_acc(Accpred, y_train))\n",
        "    ValAccuracies = []\n",
        "    Accpred = mlp_on_default.forward(X_val)\n",
        "    cat_acc = tf.metrics.CategoricalAccuracy()\n",
        "    ValAccuracies.append(cat_acc(Accpred, y_val))\n",
        "    if((epoch+1)%5 == 0):\n",
        "      print('Number of Epoch = {} - Average Loss:= {} Validation Accuracy:= {} Training Accuracy:= {}'.format(epoch + 1, np.sum(loss_total_gpu) / X_train.shape[0],np.mean(ValAccuracies)*100,np.mean(accuracies)*100))\n",
        "\n",
        "  time_taken = time.time() - time_start\n",
        "  print('\\nTotal time taken (in seconds): {:.2f}'.format(time_taken))\n",
        "\n",
        "  #inference\n",
        "  test_loss_total = tf.Variable(0, dtype=tf.float32)\n",
        "  #test_loss_total = 0.0\n",
        "  test_ds = tf.data.Dataset.from_tensor_slices((X_test, y_test)).shuffle(25, seed=epoch*(x)).batch(64)\n",
        "  cat_acc = tf.metrics.CategoricalAccuracy()\n",
        "  accuracies = []\n",
        "\n",
        "  for inputs, outputs in test_ds:\n",
        "    preds = mlp_on_default.forward(inputs)\n",
        "    test_loss_total = test_loss_total + mlp_on_default.loss(preds, outputs)\n",
        "    cat_acc = tf.metrics.CategoricalAccuracy()  \n",
        "    accuracies.append(cat_acc(preds, outputs))\n",
        "\n",
        "\n",
        "  print('Inference Loss: {:.4f}'.format(np.sum(test_loss_total.numpy()) / X_test.shape[0]))\n",
        "  print('Inference Accuracy: {:.4f}'.format(np.mean(accuracies)*100))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PXe-2MENCOjq"
      },
      "source": [
        "## One Step Inference"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "L2Regularization_MNIST.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}