{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AnkurMali/IST597_Spring_2022/blob/main/IST597_MLP_collab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "71kdFp0QgF4K"
      },
      "source": [
        "# IST597:- Multi-Layer Perceptron"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C2yHcl5xgPV1"
      },
      "source": [
        "## Load the libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2DPwxLR2gSLC"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import time\n",
        "import tensorflow as tf\n",
        "np.random.seed(1234)\n",
        "tf.random.set_seed(1234)\n",
        "from tensorflow.keras.datasets import fashion_mnist\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import LabelBinarizer\n",
        "from sklearn.metrics import roc_auc_score, accuracy_score\n",
        "from sklearn.metrics import precision_score, accuracy_score\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wV-3kEaggcO8",
        "outputId": "78b0ec51-fd7a-4975-a1d5-728a22c13b79"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[]"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "tf.config.list_physical_devices('GPU')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iw78jw6pDqSM"
      },
      "source": [
        "#Get number of Gpu's and id's in the system or else you can also use Nvidia-smi in command prompt."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Dk_S2TMg_6_"
      },
      "source": [
        "## Generate random data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "40XlFnwho7D8"
      },
      "outputs": [],
      "source": [
        "size_input = 784\n",
        "size_hidden1 = 256\n",
        "size_hidden2 = 128\n",
        "size_output = 10\n",
        "\n",
        "number_of_train_examples = 6000\n",
        "number_of_test_examples = 1000"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 693
        },
        "id": "qm23CzRihaW0",
        "outputId": "6643a50e-63b7-477e-a858-4b2896bb08fc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz\n",
            "32768/29515 [=================================] - 0s 0us/step\n",
            "40960/29515 [=========================================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz\n",
            "26427392/26421880 [==============================] - 0s 0us/step\n",
            "26435584/26421880 [==============================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz\n",
            "16384/5148 [===============================================================================================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz\n",
            "4423680/4422102 [==============================] - 0s 0us/step\n",
            "4431872/4422102 [==============================] - 0s 0us/step\n",
            "(50000, 28, 28) (50000,)\n",
            "Train dimension:\n",
            "(50000, 784)\n",
            "Test dimension:\n",
            "(10000, 784)\n",
            "Val dimension:\n",
            "(10000, 784)\n",
            "Train labels dimension:\n",
            "(50000, 10)\n",
            "Test labels dimension:\n",
            "(10000, 10)\n",
            "Test labels dimension:\n",
            "(10000, 10)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "'''\n",
        "X_train = np.random.randn(number_of_train_examples , size_input)\n",
        "y_train = np.random.randn(number_of_train_examples)\n",
        "X_test = np.random.randn(number_of_test_examples, size_input)\n",
        "y_test = np.random.randn(number_of_test_examples)\n",
        "'''\n",
        "\n",
        "\n",
        "def load_dataset(flatten=False):\n",
        "    (X_train, y_train), (X_test, y_test) = fashion_mnist.load_data()\n",
        "    # normalize x\n",
        "    X_train = X_train.astype(float)/255.0\n",
        "    X_test = X_test.astype(float)/255.0\n",
        "    # we reserve the last 10000 training examples for validation\n",
        "    X_train, X_val = X_train[:-10000], X_train[-10000:]\n",
        "    y_train, y_val = y_train[:-10000], y_train[-10000:]\n",
        "    if flatten:\n",
        "        X_train = X_train.reshape([X_train.shape[0], -1])\n",
        "        X_val = X_val.reshape([X_val.shape[0], -1])\n",
        "        X_test = X_test.reshape([X_test.shape[0], -1])\n",
        "    return X_train, y_train, X_val, y_val, X_test, y_test\n",
        "\n",
        "X_train, y_train, X_val, y_val, X_test, y_test = load_dataset()\n",
        "## Printing dimensions\n",
        "print(X_train.shape, y_train.shape)\n",
        "## Visualizing the first digit\n",
        "plt.imshow(X_train[0], cmap=\"Greys\");\n",
        "## Changing dimension of input images from N*28*28 to  N*784\n",
        "X_train = X_train.reshape((X_train.shape[0],X_train.shape[1]*X_train.shape[2]))\n",
        "X_test = X_test.reshape((X_test.shape[0],X_test.shape[1]*X_test.shape[2]))\n",
        "X_val = X_val.reshape((X_val.shape[0],X_val.shape[1]*X_val.shape[2]))\n",
        "\n",
        "print('Train dimension:');print(X_train.shape)\n",
        "print('Test dimension:');print(X_test.shape)\n",
        "print('Val dimension:');print(X_val.shape)\n",
        "\n",
        "## Changing labels to one-hot encoded vector\n",
        "lb = LabelBinarizer()\n",
        "y_train = lb.fit_transform(y_train)\n",
        "y_test = lb.transform(y_test)\n",
        "y_val = lb.transform(y_val)\n",
        "\n",
        "print('Train labels dimension:');print(y_train.shape)\n",
        "print('Test labels dimension:');print(y_test.shape)\n",
        "print('Test labels dimension:');print(y_val.shape)\n",
        "\n",
        "\n",
        "def Accuracy(yTrue,yPred):\n",
        "    return tf.keras.metrics.sparse_categorical_accuracy(yTrue, yPred)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aigqKFFF5BM2"
      },
      "outputs": [],
      "source": [
        "# Split dataset into batches\n",
        "train_ds = tf.data.Dataset.from_tensor_slices((X_train, y_train)).batch(128)\n",
        "test_ds = tf.data.Dataset.from_tensor_slices((X_test, y_test)).batch(128)\n",
        "\n",
        "\n",
        "   "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Eb4hOoVbnzSJ"
      },
      "source": [
        "## Build MLP using Eager Execution"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ht9_qpYipgHw"
      },
      "outputs": [],
      "source": [
        "# Define class to build mlp model\n",
        "class MLP(object):\n",
        "  def __init__(self, size_input, size_hidden1,size_hidden2,size_output, device=None):\n",
        "    \"\"\"\n",
        "    size_input: int, size of input layer\n",
        "    size_hidden: int, size of hidden layer\n",
        "    size_output: int, size of output layer\n",
        "    device: str or None, either 'cpu' or 'gpu' or None. If None, the device to be used will be decided automatically during Eager Execution\n",
        "    \"\"\"\n",
        "    self.size_input, self.size_hidden1,self.size_hidden2, self.size_output, self.device =\\\n",
        "    size_input, size_hidden1, size_hidden2, size_output, device\n",
        "    \n",
        "    # Initialize weights between input layer1 and hidden layer1\n",
        "    self.W1 = tf.Variable(tf.random.normal([self.size_input, self.size_hidden1]))\n",
        "    # Initialize biases for hidden layer1\n",
        "    self.b1 = tf.Variable(tf.random.normal([1, self.size_hidden1]))\n",
        "     # Initialize weights between hidden layer1 and hidden layer2 layer\n",
        "    self.W2 = tf.Variable(tf.random.normal([self.size_hidden1, self.size_hidden2]))\n",
        "    # Initialize biases for hidden hidden layer2 layer\n",
        "    self.b2 = tf.Variable(tf.random.normal([1, self.size_hidden2]))\n",
        "    # Initialize weights between hidden layer2 and output layer\n",
        "    self.W3 = tf.Variable(tf.random.normal([self.size_hidden2, self.size_output]))\n",
        "    # Initialize biases for output layer\n",
        "    self.b3 = tf.Variable(tf.random.normal([1, self.size_output]))\n",
        "    \n",
        "    # Define variables to be updated during backpropagation\n",
        "    self.variables = [self.W1, self.W2, self.b1, self.b2,self.W3,self.b3]\n",
        "    \n",
        "  def forward(self, X):\n",
        "    \"\"\"\n",
        "    forward pass\n",
        "    X: Tensor, inputs\n",
        "    \"\"\"\n",
        "    if self.device is not None:\n",
        "      with tf.device('gpu:0' if self.device=='gpu' else 'cpu'):\n",
        "        self.y = self.compute_output(X)\n",
        "    else:\n",
        "      self.y = self.compute_output(X)\n",
        "      \n",
        "    return self.y\n",
        "  \n",
        "  def loss(self, y_pred, y_true):\n",
        "    '''\n",
        "    y_pred - Tensor of shape (batch_size, size_output)\n",
        "    y_true - Tensor of shape (batch_size, size_output)\n",
        "    '''\n",
        "    y_true_tf = tf.cast(tf.reshape(y_true, (-1, self.size_output)), dtype=tf.float32)\n",
        "    y_pred_tf = tf.cast(y_pred, dtype=tf.float32)\n",
        "    cce = tf.keras.losses.CategoricalCrossentropy()\n",
        "    #print(y_true_tf.shape,y_pred_tf.shape)\n",
        "    #print('succ',cce(y_true_tf, y_pred_tf).numpy())\n",
        "    loss = cce(y_true_tf, y_pred_tf)\n",
        "    beta = 0.01\n",
        "    # Loss function with L2 Regularization with beta=0.01\n",
        "    regularizers = tf.nn.l2_loss(self.W1) + tf.nn.l2_loss(self.W2) + tf.nn.l2_loss(self.W3)\n",
        "    loss = tf.reduce_mean(loss + beta * regularizers)\n",
        "    return loss\n",
        "    #return tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=y_pred_tf, labels=y_true_tf))\n",
        "    #return tf.losses.mean_squared_error(y_true_tf, y_pred_tf)\n",
        "  \n",
        "  def backward(self, X_train, y_train):\n",
        "    \"\"\"\n",
        "    backward pass\n",
        "    \"\"\"    \n",
        "    \n",
        "    optimizer = tf.keras.optimizers.SGD(learning_rate=1e-2)\n",
        "    \n",
        "    with tf.GradientTape() as tape:\n",
        "      predicted = self.forward(X_train)\n",
        "      current_loss = self.loss(predicted, y_train)\n",
        "    grads = tape.gradient(current_loss, self.variables)\n",
        "    optimizer.apply_gradients(zip(grads, self.variables))\n",
        "    \n",
        "    '''\n",
        "\n",
        "    with tf.GradientTape(persistent=True) as tape3:\n",
        "      with tf.GradientTape(persistent=True) as tape2:\n",
        "        with tf.GradientTape(persistent=True) as tape1:\n",
        "          predicted = self.forward(X_train)\n",
        "          current_loss = self.loss(predicted, y_train)\n",
        "        grad1 = tape1.gradient(current_loss, [self.W3,self.b3])\n",
        "        optimizer.apply_gradients(zip(grad1, [self.W3,self.b3]))\n",
        "      grad2 = tape2.gradient(grad1,[self.W2,self.b2])\n",
        "      optimizer.apply_gradients(zip(grad2, [self.W2,self.b2]))\n",
        "    grad3 = tape3.gradient(grad2, [self.W1,self.b1])\n",
        "    optimizer.apply_gradients(zip(grad3, [self.W1,self.b1]))\n",
        "    return grad3\n",
        "\n",
        "    '''\n",
        "        \n",
        "  def compute_output(self, X):\n",
        "    \"\"\"\n",
        "    Custom method to obtain output tensor during forward pass\n",
        "    \"\"\"\n",
        "    # Cast X to float32\n",
        "    X_tf = tf.cast(X, dtype=tf.float32)\n",
        "    #Remember to normalize your dataset before moving forward\n",
        "    # Compute values in hidden layer1\n",
        "    what = tf.matmul(X_tf, self.W1) + self.b1\n",
        "    hhat = tf.nn.relu(what)\n",
        "    # Compute values in hidden layer2\n",
        "    what2 = tf.matmul(hhat, self.W2) + self.b2\n",
        "    hhat2 = tf.nn.relu(what2)\n",
        "    # Compute output\n",
        "    output = tf.matmul(hhat2, self.W3) + self.b3\n",
        "    #Now consider two things , First look at inbuild loss functions if they work with softmax or not and then change this\n",
        "    #Second add tf.Softmax(output) and then return this variable\n",
        "    return tf.nn.softmax(output)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cUDFOuNk618X"
      },
      "source": [
        "## Train Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FZPVUu0YDa-_"
      },
      "outputs": [],
      "source": [
        "# Set number of epochs\n",
        "NUM_EPOCHS = 50"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xI4lsqPhB6Xi",
        "outputId": "6b981097-5432-4990-950e-ab17f053a597"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "For Seed 12 :\n",
            "Number of Epoch = 5 - Average Loss:= 9.27493625 Validation Accuracy:= 53.89999747276306 Training Accuracy:= 54.88399863243103\n",
            "Number of Epoch = 10 - Average Loss:= 4.284826875 Validation Accuracy:= 60.43999791145325 Training Accuracy:= 61.92600131034851\n",
            "Number of Epoch = 15 - Average Loss:= 1.94295265625 Validation Accuracy:= 77.13000178337097 Training Accuracy:= 78.34799885749817\n",
            "Number of Epoch = 20 - Average Loss:= 0.881813046875 Validation Accuracy:= 83.10999870300293 Training Accuracy:= 84.62799787521362\n",
            "Number of Epoch = 25 - Average Loss:= 0.4077496484375 Validation Accuracy:= 84.79999899864197 Training Accuracy:= 86.04199886322021\n",
            "Number of Epoch = 30 - Average Loss:= 0.19172658203125 Validation Accuracy:= 85.07000207901001 Training Accuracy:= 85.95399856567383\n",
            "Number of Epoch = 35 - Average Loss:= 0.09327876953125 Validation Accuracy:= 84.52000021934509 Training Accuracy:= 85.4200005531311\n",
            "Number of Epoch = 40 - Average Loss:= 0.0484092236328125 Validation Accuracy:= 83.1499993801117 Training Accuracy:= 83.97200107574463\n",
            "Number of Epoch = 45 - Average Loss:= 0.027909208984375 Validation Accuracy:= 84.36999917030334 Training Accuracy:= 85.16200184822083\n",
            "Number of Epoch = 50 - Average Loss:= 0.01851970703125 Validation Accuracy:= 84.03000235557556 Training Accuracy:= 84.86599922180176\n",
            "\n",
            "Total time taken (in seconds): 1404.58\n",
            "Inference Loss: 0.0186\n",
            "Inference Accuracy: 83.5390\n",
            "For Seed 56 :\n",
            "Number of Epoch = 5 - Average Loss:= 9.310266875 Validation Accuracy:= 38.24999928474426 Training Accuracy:= 37.96800076961517\n",
            "Number of Epoch = 10 - Average Loss:= 4.342231875 Validation Accuracy:= 38.71999979019165 Training Accuracy:= 38.798001408576965\n",
            "Number of Epoch = 15 - Average Loss:= 1.99835171875 Validation Accuracy:= 57.580000162124634 Training Accuracy:= 58.56999754905701\n",
            "Number of Epoch = 20 - Average Loss:= 0.880354921875 Validation Accuracy:= 82.4899971485138 Training Accuracy:= 84.48600172996521\n",
            "Number of Epoch = 25 - Average Loss:= 0.4069668359375 Validation Accuracy:= 83.17000269889832 Training Accuracy:= 84.64599847793579\n",
            "Number of Epoch = 30 - Average Loss:= 0.1913650390625 Validation Accuracy:= 84.8800003528595 Training Accuracy:= 86.0260009765625\n",
            "Number of Epoch = 35 - Average Loss:= 0.093182255859375 Validation Accuracy:= 85.18999814987183 Training Accuracy:= 86.0480010509491\n",
            "Number of Epoch = 40 - Average Loss:= 0.04841634765625 Validation Accuracy:= 84.46000218391418 Training Accuracy:= 85.33400297164917\n",
            "Number of Epoch = 45 - Average Loss:= 0.02793281494140625 Validation Accuracy:= 83.25999975204468 Training Accuracy:= 84.49000120162964\n",
            "Number of Epoch = 50 - Average Loss:= 0.01854513671875 Validation Accuracy:= 84.65999960899353 Training Accuracy:= 85.41600108146667\n",
            "\n",
            "Total time taken (in seconds): 1219.86\n",
            "Inference Loss: 0.0186\n",
            "Inference Accuracy: 84.0963\n",
            "For Seed 876 :\n",
            "Number of Epoch = 5 - Average Loss:= 9.298765 Validation Accuracy:= 19.480000436306 Training Accuracy:= 19.514000415802002\n",
            "Number of Epoch = 10 - Average Loss:= 4.3638175 Validation Accuracy:= 19.5700004696846 Training Accuracy:= 19.562000036239624\n",
            "Number of Epoch = 15 - Average Loss:= 2.016525 Validation Accuracy:= 48.44000041484833 Training Accuracy:= 49.074000120162964\n",
            "Number of Epoch = 20 - Average Loss:= 0.875715390625 Validation Accuracy:= 82.30999708175659 Training Accuracy:= 83.69799852371216\n",
            "Number of Epoch = 25 - Average Loss:= 0.404986484375 Validation Accuracy:= 84.53999757766724 Training Accuracy:= 85.55999994277954\n",
            "Number of Epoch = 30 - Average Loss:= 0.19047658203125 Validation Accuracy:= 84.52000021934509 Training Accuracy:= 85.57999730110168\n",
            "Number of Epoch = 35 - Average Loss:= 0.09272681640625 Validation Accuracy:= 84.42999720573425 Training Accuracy:= 85.25400161743164\n",
            "Number of Epoch = 40 - Average Loss:= 0.048162392578125 Validation Accuracy:= 84.40999984741211 Training Accuracy:= 85.51200032234192\n",
            "Number of Epoch = 45 - Average Loss:= 0.027789375 Validation Accuracy:= 82.84000158309937 Training Accuracy:= 83.7719976902008\n",
            "Number of Epoch = 50 - Average Loss:= 0.018456025390625 Validation Accuracy:= 84.07999873161316 Training Accuracy:= 85.13000011444092\n",
            "\n",
            "Total time taken (in seconds): 1217.15\n",
            "Inference Loss: 0.0185\n",
            "Inference Accuracy: 83.4693\n",
            "For Seed 6666 :\n",
            "Number of Epoch = 5 - Average Loss:= 9.282080625 Validation Accuracy:= 36.89999878406525 Training Accuracy:= 37.14999854564667\n",
            "Number of Epoch = 10 - Average Loss:= 4.3300878125 Validation Accuracy:= 37.74000108242035 Training Accuracy:= 38.01200091838837\n",
            "Number of Epoch = 15 - Average Loss:= 2.0630840625 Validation Accuracy:= 37.860000133514404 Training Accuracy:= 38.207998871803284\n",
            "Number of Epoch = 20 - Average Loss:= 0.878259453125 Validation Accuracy:= 82.37000107765198 Training Accuracy:= 83.49599838256836\n",
            "Number of Epoch = 25 - Average Loss:= 0.405730703125 Validation Accuracy:= 83.71000289916992 Training Accuracy:= 85.17000079154968\n",
            "Number of Epoch = 30 - Average Loss:= 0.1906996875 Validation Accuracy:= 83.85999798774719 Training Accuracy:= 84.97200012207031\n",
            "Number of Epoch = 35 - Average Loss:= 0.0927625390625 Validation Accuracy:= 84.76999998092651 Training Accuracy:= 85.75800061225891\n",
            "Number of Epoch = 40 - Average Loss:= 0.0481422021484375 Validation Accuracy:= 84.14999842643738 Training Accuracy:= 85.25400161743164\n",
            "Number of Epoch = 45 - Average Loss:= 0.02775811279296875 Validation Accuracy:= 84.63000059127808 Training Accuracy:= 85.38399934768677\n",
            "Number of Epoch = 50 - Average Loss:= 0.018421534423828125 Validation Accuracy:= 82.46999979019165 Training Accuracy:= 83.32800269126892\n",
            "\n",
            "Total time taken (in seconds): 1111.45\n",
            "Inference Loss: 0.0190\n",
            "Inference Accuracy: 81.7874\n",
            "For Seed 7777 :\n",
            "Number of Epoch = 5 - Average Loss:= 9.25455375 Validation Accuracy:= 27.59000062942505 Training Accuracy:= 28.091999888420105\n",
            "Number of Epoch = 10 - Average Loss:= 4.3296971875 Validation Accuracy:= 28.389999270439148 Training Accuracy:= 29.016000032424927\n",
            "Number of Epoch = 15 - Average Loss:= 2.03955125 Validation Accuracy:= 45.010000467300415 Training Accuracy:= 45.28599977493286\n",
            "Number of Epoch = 20 - Average Loss:= 0.87351015625 Validation Accuracy:= 81.58000111579895 Training Accuracy:= 83.22200179100037\n",
            "Number of Epoch = 25 - Average Loss:= 0.403841015625 Validation Accuracy:= 83.28999876976013 Training Accuracy:= 84.50199961662292\n",
            "Number of Epoch = 30 - Average Loss:= 0.18991646484375 Validation Accuracy:= 85.03000140190125 Training Accuracy:= 85.73200106620789\n",
            "Number of Epoch = 35 - Average Loss:= 0.0924522265625 Validation Accuracy:= 82.88999795913696 Training Accuracy:= 83.91000032424927\n",
            "Number of Epoch = 40 - Average Loss:= 0.0480174072265625 Validation Accuracy:= 82.40000009536743 Training Accuracy:= 83.56000185012817\n",
            "Number of Epoch = 45 - Average Loss:= 0.0277151123046875 Validation Accuracy:= 83.53999853134155 Training Accuracy:= 84.46199893951416\n",
            "Number of Epoch = 50 - Average Loss:= 0.01840648193359375 Validation Accuracy:= 84.03000235557556 Training Accuracy:= 85.04999876022339\n",
            "\n",
            "Total time taken (in seconds): 1195.75\n",
            "Inference Loss: 0.0185\n",
            "Inference Accuracy: 83.3201\n",
            "For Seed 3452 :\n",
            "Number of Epoch = 5 - Average Loss:= 9.233831875 Validation Accuracy:= 29.730001091957092 Training Accuracy:= 29.276001453399658\n",
            "Number of Epoch = 10 - Average Loss:= 4.2975078125 Validation Accuracy:= 38.7800008058548 Training Accuracy:= 38.47799897193909\n",
            "Number of Epoch = 15 - Average Loss:= 2.04792953125 Validation Accuracy:= 39.30999934673309 Training Accuracy:= 39.11600112915039\n",
            "Number of Epoch = 20 - Average Loss:= 0.871906875 Validation Accuracy:= 82.22000002861023 Training Accuracy:= 83.25999975204468\n",
            "Number of Epoch = 25 - Average Loss:= 0.4028781640625 Validation Accuracy:= 84.579998254776 Training Accuracy:= 85.79999804496765\n",
            "Number of Epoch = 30 - Average Loss:= 0.189407890625 Validation Accuracy:= 84.85000133514404 Training Accuracy:= 85.81799864768982\n",
            "Number of Epoch = 35 - Average Loss:= 0.0921826171875 Validation Accuracy:= 84.81000065803528 Training Accuracy:= 85.83400249481201\n",
            "Number of Epoch = 40 - Average Loss:= 0.04788349609375 Validation Accuracy:= 83.92000198364258 Training Accuracy:= 85.20600199699402\n",
            "Number of Epoch = 45 - Average Loss:= 0.02765926513671875 Validation Accuracy:= 82.95999765396118 Training Accuracy:= 83.96400213241577\n",
            "Number of Epoch = 50 - Average Loss:= 0.018392276611328126 Validation Accuracy:= 81.7799985408783 Training Accuracy:= 82.76600241661072\n",
            "\n",
            "Total time taken (in seconds): 1051.19\n",
            "Inference Loss: 0.0192\n",
            "Inference Accuracy: 80.9614\n",
            "For Seed 5 :\n",
            "Number of Epoch = 5 - Average Loss:= 9.256710625 Validation Accuracy:= 38.44999969005585 Training Accuracy:= 37.869998812675476\n",
            "Number of Epoch = 10 - Average Loss:= 4.2965115625 Validation Accuracy:= 47.85999953746796 Training Accuracy:= 47.227999567985535\n",
            "Number of Epoch = 15 - Average Loss:= 1.94789546875 Validation Accuracy:= 74.68000054359436 Training Accuracy:= 76.20400190353394\n",
            "Number of Epoch = 20 - Average Loss:= 0.87559640625 Validation Accuracy:= 83.81999731063843 Training Accuracy:= 85.25599837303162\n",
            "Number of Epoch = 25 - Average Loss:= 0.4049382421875 Validation Accuracy:= 84.06000137329102 Training Accuracy:= 85.64599752426147\n",
            "Number of Epoch = 30 - Average Loss:= 0.19044013671875 Validation Accuracy:= 84.25999879837036 Training Accuracy:= 85.316002368927\n",
            "Number of Epoch = 35 - Average Loss:= 0.0927011328125 Validation Accuracy:= 84.53999757766724 Training Accuracy:= 85.46000123023987\n",
            "Number of Epoch = 40 - Average Loss:= 0.0481359814453125 Validation Accuracy:= 83.39999914169312 Training Accuracy:= 84.82199907302856\n",
            "Number of Epoch = 45 - Average Loss:= 0.027783037109375 Validation Accuracy:= 84.07999873161316 Training Accuracy:= 85.22599935531616\n",
            "Number of Epoch = 50 - Average Loss:= 0.018460250244140625 Validation Accuracy:= 84.21000242233276 Training Accuracy:= 84.95799899101257\n",
            "\n",
            "Total time taken (in seconds): 1196.72\n",
            "Inference Loss: 0.0187\n",
            "Inference Accuracy: 83.3698\n"
          ]
        }
      ],
      "source": [
        "seed = [12,56,876,6666,7777,3452,5]\n",
        "\n",
        "for x in seed:\n",
        "  print('For Seed',x,':')\n",
        "\n",
        "  #Default mode\n",
        "  mlp_on_default = MLP(size_input, size_hidden1, size_hidden2, size_output, device='tpu')\n",
        "  #hidden2 = MLP(size_hidden2, size_hidden2, size_output, device='gpu')\n",
        "  time_start = time.time()\n",
        "\n",
        "  for epoch in range(NUM_EPOCHS):\n",
        "    loss_total_gpu = tf.zeros([1,1], dtype=tf.float32)\n",
        "    lt = 0\n",
        "    acc = 0\n",
        "    train_ds = tf.data.Dataset.from_tensor_slices((X_train, y_train)).shuffle(25, seed=epoch*(x)).batch(64)\n",
        "    accuracies = []\n",
        "    for inputs, outputs in train_ds:\n",
        "      preds = mlp_on_default.forward(inputs)\n",
        "      loss_total_gpu = loss_total_gpu + mlp_on_default.loss(preds, outputs)\n",
        "      acc = loss_total_gpu + mlp_on_default.loss(preds, outputs)\n",
        "      lt = lt + mlp_on_default.loss(preds, outputs)\n",
        "      mlp_on_default.backward(inputs, outputs)\n",
        "      \n",
        "    Accpred = mlp_on_default.forward(X_train)\n",
        "    cat_acc = tf.metrics.CategoricalAccuracy()\n",
        "    accuracies.append(cat_acc(Accpred, y_train))\n",
        "    ValAccuracies = []\n",
        "    Accpred = mlp_on_default.forward(X_val)\n",
        "    cat_acc = tf.metrics.CategoricalAccuracy()\n",
        "    ValAccuracies.append(cat_acc(Accpred, y_val))\n",
        "    if((epoch+1)%5 == 0):\n",
        "      print('Number of Epoch = {} - Average Loss:= {} Validation Accuracy:= {} Training Accuracy:= {}'.format(epoch + 1, np.sum(loss_total_gpu) / X_train.shape[0],np.mean(ValAccuracies)*100,np.mean(accuracies)*100))\n",
        "\n",
        "  time_taken = time.time() - time_start\n",
        "  print('\\nTotal time taken (in seconds): {:.2f}'.format(time_taken))\n",
        "\n",
        "  #inference\n",
        "  test_loss_total = tf.Variable(0, dtype=tf.float32)\n",
        "  #test_loss_total = 0.0\n",
        "  test_ds = tf.data.Dataset.from_tensor_slices((X_test, y_test)).shuffle(25, seed=epoch*(x)).batch(64)\n",
        "  cat_acc = tf.metrics.CategoricalAccuracy()\n",
        "  accuracies = []\n",
        "\n",
        "  for inputs, outputs in test_ds:\n",
        "    preds = mlp_on_default.forward(inputs)\n",
        "    test_loss_total = test_loss_total + mlp_on_default.loss(preds, outputs)\n",
        "    cat_acc = tf.metrics.CategoricalAccuracy()  \n",
        "    accuracies.append(cat_acc(preds, outputs))\n",
        "\n",
        "\n",
        "  print('Inference Loss: {:.4f}'.format(np.sum(test_loss_total.numpy()) / X_test.shape[0]))\n",
        "  print('Inference Accuracy: {:.4f}'.format(np.mean(accuracies)*100))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PXe-2MENCOjq"
      },
      "source": [
        "## One Step Inference"
      ]
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "collapsed_sections": [],
      "name": "L2Regularization_FashionMNIST.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}