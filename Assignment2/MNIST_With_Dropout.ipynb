{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AnkurMali/IST597_Spring_2022/blob/main/IST597_MLP_collab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "71kdFp0QgF4K"
      },
      "source": [
        "# IST597:- Multi-Layer Perceptron"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C2yHcl5xgPV1"
      },
      "source": [
        "## Load the libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2DPwxLR2gSLC"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import time\n",
        "import tensorflow as tf\n",
        "np.random.seed(1234)\n",
        "tf.random.set_seed(1234)\n",
        "from tensorflow.keras.datasets import mnist\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import LabelBinarizer\n",
        "from sklearn.metrics import roc_auc_score, accuracy_score\n",
        "from sklearn.metrics import precision_score, accuracy_score\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wV-3kEaggcO8",
        "outputId": "efba556e-1bfb-4c1c-af1c-5d1c2224c915"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[]"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "tf.config.list_physical_devices('GPU')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iw78jw6pDqSM"
      },
      "source": [
        "#Get number of Gpu's and id's in the system or else you can also use Nvidia-smi in command prompt."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Dk_S2TMg_6_"
      },
      "source": [
        "## Generate random data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "40XlFnwho7D8"
      },
      "outputs": [],
      "source": [
        "size_input = 784\n",
        "size_hidden1 = 256\n",
        "size_hidden2 = 128\n",
        "size_output = 10\n",
        "\n",
        "number_of_train_examples = 6000\n",
        "number_of_test_examples = 1000"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 497
        },
        "id": "qm23CzRihaW0",
        "outputId": "0f89e8ae-0a7c-45eb-9988-05e3a916f3ed"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(50000, 28, 28) (50000,)\n",
            "Train dimension:\n",
            "(50000, 784)\n",
            "Test dimension:\n",
            "(10000, 784)\n",
            "Val dimension:\n",
            "(10000, 784)\n",
            "Train labels dimension:\n",
            "(50000, 10)\n",
            "Test labels dimension:\n",
            "(10000, 10)\n",
            "Test labels dimension:\n",
            "(10000, 10)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAD4CAYAAAAq5pAIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAOTklEQVR4nO3dfYxUZZbH8d8RQVSIQWk7xCHbsxM1MSbTgyVZw0tYxiXIP2AwZkicsJFsT3xJBkPMGDZxfEkMMcuMGM0kPQvCbGYdRwHBxOyihMSQ6GipqIDvpgmNvDRRGSHKLHD2j75MWqx6qqm6Vbfo8/0knaq6p27fQ8GPW3Wfe+sxdxeAke+8ohsA0BqEHQiCsANBEHYgCMIOBHF+Kzc2ceJE7+rqauUmgVD6+vp0+PBhq1RrKOxmNlfSKkmjJP2nu69IPb+rq0vlcrmRTQJIKJVKVWt1v403s1GSnpR0k6RrJC0ys2vq/X0AmquRz+xTJX3i7p+5+98k/UnS/HzaApC3RsJ+haS9Qx73Z8u+w8x6zKxsZuWBgYEGNgegEU0/Gu/uve5ecvdSR0dHszcHoIpGwr5P0uQhj3+QLQPQhhoJ+xuSrjSzH5rZGEk/k7Q5n7YA5K3uoTd3P2Fmd0v6Xw0Ova1x9125dQYgVw2Ns7v7i5JezKkXAE3E6bJAEIQdCIKwA0EQdiAIwg4EQdiBIAg7EARhB4Ig7EAQhB0IgrADQRB2IAjCDgRB2IEgCDsQBGEHgiDsQBCEHQiCsANBEHYgCMIOBEHYgSAIOxAEYQeCIOxAEIQdCIKwA0EQdiAIwg4E0dAsrmh/p06dStaPHz/e1O2vW7euau3YsWPJdXfv3p2sP/bYY8n68uXLq9aeeOKJ5LoXXnhhsr5y5cpk/Y477kjWi9BQ2M2sT9LXkk5KOuHupTyaApC/PPbs/+zuh3P4PQCaiM/sQBCNht0lbTGzN82sp9ITzKzHzMpmVh4YGGhwcwDq1WjYp7v7FEk3SbrLzGae+QR373X3kruXOjo6GtwcgHo1FHZ335fdHpK0UdLUPJoCkL+6w25mF5vZ+NP3Jc2RtDOvxgDkq5Gj8Z2SNprZ6d/z3+7+P7l0NcIcOXIkWT958mSy/s477yTrW7ZsqVr76quvkuv29vYm60Xq6upK1pctW5asr169umrtkksuSa47Y8aMZH327NnJejuqO+zu/pmkH+fYC4AmYugNCIKwA0EQdiAIwg4EQdiBILjENQf9/f3Jend3d7L+5Zdf5tnOOeO889L7mtTQmVT7MtQlS5ZUrV1++eXJdceNG5esn4tng7JnB4Ig7EAQhB0IgrADQRB2IAjCDgRB2IEgGGfPwWWXXZasd3Z2JuvtPM4+Z86cZL3Wn33Dhg1VaxdccEFy3VmzZiXrODvs2YEgCDsQBGEHgiDsQBCEHQiCsANBEHYgCMbZc1Druuq1a9cm688991yyfsMNNyTrCxcuTNZTpk+fnqxv2rQpWR8zZkyyfuDAgaq1VatWJddFvtizA0EQdiAIwg4EQdiBIAg7EARhB4Ig7EAQ5u4t21ipVPJyudyy7Z0rjh8/nqzXGstevnx51dqjjz6aXHfbtm3J+syZM5N1tJdSqaRyuWyVajX37Ga2xswOmdnOIcsuNbOXzOzj7HZCng0DyN9w3savlTT3jGX3Sdrq7ldK2po9BtDGaobd3V+R9MUZi+dLWpfdXydpQc59AchZvQfoOt19f3b/gKSqX7JmZj1mVjaz8sDAQJ2bA9Coho/G++ARvqpH+dy9191L7l46FyfDA0aKesN+0MwmSVJ2eyi/lgA0Q71h3yxpcXZ/saT0dZAAClfzenYze1rSLEkTzaxf0q8lrZD0ZzNbImmPpFub2eRIV+v702uZMKH+kc/HH388WZ8xY0ayblZxSBdtqGbY3X1RldJPc+4FQBNxuiwQBGEHgiDsQBCEHQiCsANB8FXSI8DSpUur1l5//fXkuhs3bkzWd+3alaxfe+21yTraB3t2IAjCDgRB2IEgCDsQBGEHgiDsQBCEHQiCcfYRIPVV0729vcl1t27dmqzPnz8/WV+wIP31g9OmTatau/nmm5PrcvlsvtizA0EQdiAIwg4EQdiBIAg7EARhB4Ig7EAQTNkcXK3r3efOPXNOz+86cuRI3dtes2ZNsr5w4cJkfdy4cXVve6RqaMpmACMDYQeCIOxAEIQdCIKwA0EQdiAIwg4EwfXswU2dOjVZr/W98ffcc0+y/uyzz1at3X777cl1P/3002T93nvvTdbHjx+frEdTc89uZmvM7JCZ7Ryy7AEz22dmO7Kfec1tE0CjhvM2fq2kSqdR/dbdu7OfF/NtC0Deaobd3V+R9EULegHQRI0coLvbzN7N3uZPqPYkM+sxs7KZlQcGBhrYHIBG1Bv230n6kaRuSfslraz2RHfvdfeSu5c6Ojrq3ByARtUVdnc/6O4n3f2UpN9LSh/SBVC4usJuZpOGPLxZ0s5qzwXQHmpez25mT0uaJWmipIOSfp097pbkkvok/cLd99faGNezjzzffvttsv7aa69Vrd14443JdWv927zllluS9WeeeSZZH4lS17PXPKnG3RdVWLy64a4AtBSnywJBEHYgCMIOBEHYgSAIOxAEl7iiIWPHjk3WZ82aVbU2atSo5LonTpxI1p9//vlk/cMPP6xau/rqq5PrjkTs2YEgCDsQBGEHgiDsQBCEHQiCsANBEHYgCMbZkfT5558n6xs2bEjWX3311aq1WuPotVx//fXJ+lVXXdXQ7x9p2LMDQRB2IAjCDgRB2IEgCDsQBGEHgiDsQBCMs49wtabcevLJJ5P1p556Klnv7+8/656Gq9b17l1dXcm6WcVvVA6LPTsQBGEHgiDsQBCEHQiCsANBEHYgCMIOBME4+zng6NGjyfoLL7xQtfbQQw8l1/3oo4/q6ikPs2fPTtZXrFiRrF933XV5tjPi1dyzm9lkM9tmZrvNbJeZ/TJbfqmZvWRmH2e3E5rfLoB6Dedt/AlJy9z9Gkn/JOkuM7tG0n2Strr7lZK2Zo8BtKmaYXf3/e7+Vnb/a0nvS7pC0nxJ67KnrZO0oFlNAmjcWR2gM7MuST+R9BdJne6+PysdkNRZZZ0eMyubWbnWedoAmmfYYTezcZLWS1rq7n8dWnN3l+SV1nP3XncvuXupo6OjoWYB1G9YYTez0RoM+h/d/fTXiR40s0lZfZKkQ81pEUAeag692eB1gqslve/uvxlS2ixpsaQV2e2mpnQ4Ahw7dixZ37t3b7J+2223Jetvv/32WfeUlzlz5iTrDz74YNVara+C5hLVfA1nnH2apJ9Les/MdmTLlmsw5H82syWS9ki6tTktAshDzbC7+3ZJ1f6L/Wm+7QBoFk6XBYIg7EAQhB0IgrADQRB2IAgucR2mb775pmpt6dKlyXW3b9+erH/wwQd19ZSHefPmJev3339/st7d3Z2sjx49+qx7QnOwZweCIOxAEIQdCIKwA0EQdiAIwg4EQdiBIMKMs/f19SXrjzzySLL+8ssvV63t2bOnnpZyc9FFF1WtPfzww8l177zzzmR9zJgxdfWE9sOeHQiCsANBEHYgCMIOBEHYgSAIOxAEYQeCCDPOvn79+mR99erVTdv2lClTkvVFixYl6+efn/5r6unpqVobO3Zscl3EwZ4dCIKwA0EQdiAIwg4EQdiBIAg7EARhB4Iwd08/wWyypD9I6pTkknrdfZWZPSDp3yQNZE9d7u4vpn5XqVTycrnccNMAKiuVSiqXyxVnXR7OSTUnJC1z97fMbLykN83spaz2W3f/j7waBdA8w5mffb+k/dn9r83sfUlXNLsxAPk6q8/sZtYl6SeS/pItutvM3jWzNWY2oco6PWZWNrPywMBApacAaIFhh93MxklaL2mpu/9V0u8k/UhStwb3/Csrrefuve5ecvdSR0dHDi0DqMewwm5mozUY9D+6+wZJcveD7n7S3U9J+r2kqc1rE0CjaobdzEzSaknvu/tvhiyfNORpN0vamX97APIynKPx0yT9XNJ7ZrYjW7Zc0iIz69bgcFyfpF80pUMAuRjO0fjtkiqN2yXH1AG0F86gA4Ig7EAQhB0IgrADQRB2IAjCDgRB2IEgCDsQBGEHgiDsQBCEHQiCsANBEHYgCMIOBFHzq6Rz3ZjZgKQ9QxZNlHS4ZQ2cnXbtrV37kuitXnn29g/uXvH731oa9u9t3Kzs7qXCGkho197atS+J3urVqt54Gw8EQdiBIIoOe2/B209p197atS+J3urVkt4K/cwOoHWK3rMDaBHCDgRRSNjNbK6ZfWhmn5jZfUX0UI2Z9ZnZe2a2w8wKnV86m0PvkJntHLLsUjN7ycw+zm4rzrFXUG8PmNm+7LXbYWbzCuptspltM7PdZrbLzH6ZLS/0tUv01ZLXreWf2c1slKSPJP2LpH5Jb0ha5O67W9pIFWbWJ6nk7oWfgGFmMyUdlfQHd782W/aopC/cfUX2H+UEd/9Vm/T2gKSjRU/jnc1WNGnoNOOSFkj6VxX42iX6ulUteN2K2LNPlfSJu3/m7n+T9CdJ8wvoo+25+yuSvjhj8XxJ67L76zT4j6XlqvTWFtx9v7u/ld3/WtLpacYLfe0SfbVEEWG/QtLeIY/71V7zvbukLWb2ppn1FN1MBZ3uvj+7f0BSZ5HNVFBzGu9WOmOa8bZ57eqZ/rxRHKD7vunuPkXSTZLuyt6utiUf/AzWTmOnw5rGu1UqTDP+d0W+dvVOf96oIsK+T9LkIY9/kC1rC+6+L7s9JGmj2m8q6oOnZ9DNbg8V3M/ftdM03pWmGVcbvHZFTn9eRNjfkHSlmf3QzMZI+pmkzQX08T1mdnF24ERmdrGkOWq/qag3S1qc3V8saVOBvXxHu0zjXW2acRX82hU+/bm7t/xH0jwNHpH/VNK/F9FDlb7+UdI72c+uonuT9LQG39b9nwaPbSyRdJmkrZI+lvSypEvbqLf/kvSepHc1GKxJBfU2XYNv0d+VtCP7mVf0a5foqyWvG6fLAkFwgA4IgrADQRB2IAjCDgRB2IEgCDsQBGEHgvh//v1TaNV8b54AAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "'''\n",
        "X_train = np.random.randn(number_of_train_examples , size_input)\n",
        "y_train = np.random.randn(number_of_train_examples)\n",
        "X_test = np.random.randn(number_of_test_examples, size_input)\n",
        "y_test = np.random.randn(number_of_test_examples)\n",
        "'''\n",
        "\n",
        "\n",
        "def load_dataset(flatten=False):\n",
        "    (X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
        "    # normalize x\n",
        "    X_train = X_train.astype(float)/255.0\n",
        "    X_test = X_test.astype(float)/255.0\n",
        "    # we reserve the last 10000 training examples for validation\n",
        "    X_train, X_val = X_train[:-10000], X_train[-10000:]\n",
        "    y_train, y_val = y_train[:-10000], y_train[-10000:]\n",
        "    if flatten:\n",
        "        X_train = X_train.reshape([X_train.shape[0], -1])\n",
        "        X_val = X_val.reshape([X_val.shape[0], -1])\n",
        "        X_test = X_test.reshape([X_test.shape[0], -1])\n",
        "    return X_train, y_train, X_val, y_val, X_test, y_test\n",
        "\n",
        "X_train, y_train, X_val, y_val, X_test, y_test = load_dataset()\n",
        "## Printing dimensions\n",
        "print(X_train.shape, y_train.shape)\n",
        "## Visualizing the first digit\n",
        "plt.imshow(X_train[0], cmap=\"Greys\");\n",
        "## Changing dimension of input images from N*28*28 to  N*784\n",
        "X_train = X_train.reshape((X_train.shape[0],X_train.shape[1]*X_train.shape[2]))\n",
        "X_test = X_test.reshape((X_test.shape[0],X_test.shape[1]*X_test.shape[2]))\n",
        "X_val = X_val.reshape((X_val.shape[0],X_val.shape[1]*X_val.shape[2]))\n",
        "\n",
        "print('Train dimension:');print(X_train.shape)\n",
        "print('Test dimension:');print(X_test.shape)\n",
        "print('Val dimension:');print(X_val.shape)\n",
        "\n",
        "## Changing labels to one-hot encoded vector\n",
        "lb = LabelBinarizer()\n",
        "y_train = lb.fit_transform(y_train)\n",
        "y_test = lb.transform(y_test)\n",
        "y_val = lb.transform(y_val)\n",
        "\n",
        "print('Train labels dimension:');print(y_train.shape)\n",
        "print('Test labels dimension:');print(y_test.shape)\n",
        "print('Test labels dimension:');print(y_val.shape)\n",
        "\n",
        "\n",
        "def Accuracy(yTrue,yPred):\n",
        "    return tf.keras.metrics.sparse_categorical_accuracy(yTrue, yPred)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aigqKFFF5BM2"
      },
      "outputs": [],
      "source": [
        "# Split dataset into batches\n",
        "train_ds = tf.data.Dataset.from_tensor_slices((X_train, y_train)).batch(128)\n",
        "test_ds = tf.data.Dataset.from_tensor_slices((X_test, y_test)).batch(128)\n",
        "\n",
        "\n",
        "   "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Eb4hOoVbnzSJ"
      },
      "source": [
        "## Build MLP using Eager Execution"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ht9_qpYipgHw"
      },
      "outputs": [],
      "source": [
        "# Define class to build mlp model\n",
        "class MLP(object):\n",
        "  def __init__(self, size_input, size_hidden1,size_hidden2,size_output, device=None):\n",
        "    \"\"\"\n",
        "    size_input: int, size of input layer\n",
        "    size_hidden: int, size of hidden layer\n",
        "    size_output: int, size of output layer\n",
        "    device: str or None, either 'cpu' or 'gpu' or None. If None, the device to be used will be decided automatically during Eager Execution\n",
        "    \"\"\"\n",
        "    self.size_input, self.size_hidden1,self.size_hidden2, self.size_output, self.device =\\\n",
        "    size_input, size_hidden1, size_hidden2, size_output, device\n",
        "    \n",
        "    # Initialize weights between input layer and hidden layer\n",
        "    self.W1 = tf.Variable(tf.random.normal([self.size_input, self.size_hidden1]))\n",
        "    # Initialize biases for hidden layer\n",
        "    self.b1 = tf.Variable(tf.random.normal([1, self.size_hidden1]))\n",
        "     # Initialize weights between hidden layer1 and hidden layer2 layer\n",
        "    self.W2 = tf.Variable(tf.random.normal([self.size_hidden1, self.size_hidden2]))\n",
        "    # Initialize biases for hidden layer1 and hidden layer2 layer\n",
        "    self.b2 = tf.Variable(tf.random.normal([1, self.size_hidden2]))\n",
        "    # Initialize weights between hidden layer1 and hidden layer2 layer\n",
        "    self.W3 = tf.Variable(tf.random.normal([self.size_hidden2, self.size_output]))\n",
        "    # Initialize biases for hidden layer1 and hidden layer2 layer\n",
        "    self.b3 = tf.Variable(tf.random.normal([1, self.size_output]))\n",
        "    \n",
        "    # Define variables to be updated during backpropagation\n",
        "    self.variables = [self.W1, self.W2, self.b1, self.b2,self.W3,self.b3]\n",
        "    \n",
        "  def forward(self, X,Training=False):\n",
        "    \"\"\"\n",
        "    forward pass\n",
        "    X: Tensor, inputs\n",
        "    \"\"\"\n",
        "    if self.device is not None:\n",
        "      with tf.device('gpu:0' if self.device=='gpu' else 'cpu'):\n",
        "        self.y = self.compute_output(X,Training)\n",
        "    else:\n",
        "      self.y = self.compute_output(X,Training)\n",
        "      \n",
        "    return self.y\n",
        "  \n",
        "  def loss(self, y_pred, y_true):\n",
        "    '''\n",
        "    y_pred - Tensor of shape (batch_size, size_output)\n",
        "    y_true - Tensor of shape (batch_size, size_output)\n",
        "    '''\n",
        "    y_true_tf = tf.cast(tf.reshape(y_true, (-1, self.size_output)), dtype=tf.float32)\n",
        "    y_pred_tf = tf.cast(y_pred, dtype=tf.float32)\n",
        "    cce = tf.keras.losses.CategoricalCrossentropy()\n",
        "    #print(y_true_tf.shape,y_pred_tf.shape)\n",
        "    #print('succ',cce(y_true_tf, y_pred_tf).numpy())\n",
        "    return cce(y_true_tf, y_pred_tf)\n",
        "    #return tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=y_pred_tf, labels=y_true_tf))\n",
        "    #return tf.losses.mean_squared_error(y_true_tf, y_pred_tf)\n",
        "  \n",
        "  def backward(self, X_train, y_train):\n",
        "    \"\"\"\n",
        "    backward pass\n",
        "    \"\"\"    \n",
        "    \n",
        "    optimizer = tf.keras.optimizers.SGD(learning_rate=1e-2)\n",
        "    \n",
        "    with tf.GradientTape() as tape:\n",
        "      predicted = self.forward(X_train)\n",
        "      current_loss = self.loss(predicted, y_train)\n",
        "    grads = tape.gradient(current_loss, self.variables)\n",
        "    optimizer.apply_gradients(zip(grads, self.variables))\n",
        "    \n",
        "    '''\n",
        "\n",
        "    with tf.GradientTape(persistent=True) as tape3:\n",
        "      with tf.GradientTape(persistent=True) as tape2:\n",
        "        with tf.GradientTape(persistent=True) as tape1:\n",
        "          predicted = self.forward(X_train)\n",
        "          current_loss = self.loss(predicted, y_train)\n",
        "        grad1 = tape1.gradient(current_loss, [self.W3,self.b3])\n",
        "        optimizer.apply_gradients(zip(grad1, [self.W3,self.b3]))\n",
        "      grad2 = tape2.gradient(grad1,[self.W2,self.b2])\n",
        "      optimizer.apply_gradients(zip(grad2, [self.W2,self.b2]))\n",
        "    grad3 = tape3.gradient(grad2, [self.W1,self.b1])\n",
        "    optimizer.apply_gradients(zip(grad3, [self.W1,self.b1]))\n",
        "    return grad3\n",
        "\n",
        "    '''\n",
        "        \n",
        "  def compute_output(self, X,Training=False):\n",
        "    \"\"\"\n",
        "    Custom method to obtain output tensor during forward pass\n",
        "    \"\"\"\n",
        "    # Cast X to float32\n",
        "    X_tf = tf.cast(X, dtype=tf.float32)\n",
        "    #Remember to normalize your dataset before moving forward\n",
        "    # Compute values in hidden layer1\n",
        "    what = tf.matmul(X_tf, self.W1) + self.b1\n",
        "    hhat = tf.nn.relu(what)\n",
        "    if(Training):\n",
        "      hhat = tf.nn.dropout(hhat, 0.5)\n",
        "    # Compute values in hidden layer2\n",
        "    what2 = tf.matmul(hhat, self.W2) + self.b2\n",
        "    hhat2 = tf.nn.relu(what2)\n",
        "    if(Training):\n",
        "      hhat2 = tf.nn.dropout(hhat2, 0.5)\n",
        "\n",
        "    # Compute output\n",
        "    output = tf.matmul(hhat2, self.W3) + self.b3\n",
        "    #Now consider two things , First look at inbuild loss functions if they work with softmax or not and then change this\n",
        "    #Second add tf.Softmax(output) and then return this variable\n",
        "    return tf.nn.softmax(output)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cUDFOuNk618X"
      },
      "source": [
        "## Train Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FZPVUu0YDa-_"
      },
      "outputs": [],
      "source": [
        "# Set number of epochs\n",
        "NUM_EPOCHS = 100"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "xI4lsqPhB6Xi",
        "outputId": "d213952e-a64c-4a7a-b983-5282929f1fa1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "For Seed 56 :\n",
            "Number of Epoch = 5 - Average Loss:= 0.19205939453125 Validation Accuracy:= 51.05999708175659 Training Accuracy:= 52.056002616882324\n",
            "Number of Epoch = 10 - Average Loss:= 0.1804155078125 Validation Accuracy:= 55.21000027656555 Training Accuracy:= 55.72599768638611\n",
            "Number of Epoch = 15 - Average Loss:= 0.175452734375 Validation Accuracy:= 55.970001220703125 Training Accuracy:= 56.51000142097473\n",
            "Number of Epoch = 20 - Average Loss:= 0.17415708984375 Validation Accuracy:= 56.37999773025513 Training Accuracy:= 56.94800019264221\n",
            "Number of Epoch = 25 - Average Loss:= 0.1699783984375 Validation Accuracy:= 64.3999993801117 Training Accuracy:= 64.82200026512146\n",
            "Number of Epoch = 30 - Average Loss:= 0.16621732421875 Validation Accuracy:= 64.82999920845032 Training Accuracy:= 65.3760015964508\n",
            "Number of Epoch = 35 - Average Loss:= 0.16236884765625 Validation Accuracy:= 64.99999761581421 Training Accuracy:= 65.53800106048584\n",
            "Number of Epoch = 40 - Average Loss:= 0.161231416015625 Validation Accuracy:= 65.6499981880188 Training Accuracy:= 66.20399951934814\n",
            "Number of Epoch = 45 - Average Loss:= 0.160578203125 Validation Accuracy:= 65.2899980545044 Training Accuracy:= 65.79999923706055\n",
            "Number of Epoch = 50 - Average Loss:= 0.159633623046875 Validation Accuracy:= 65.35000205039978 Training Accuracy:= 66.04599952697754\n",
            "Number of Epoch = 55 - Average Loss:= 0.159238740234375 Validation Accuracy:= 65.65999984741211 Training Accuracy:= 66.28199815750122\n",
            "Number of Epoch = 60 - Average Loss:= 0.159403173828125 Validation Accuracy:= 66.35000109672546 Training Accuracy:= 67.08199977874756\n",
            "Number of Epoch = 65 - Average Loss:= 0.1564486328125 Validation Accuracy:= 66.29999876022339 Training Accuracy:= 66.98200106620789\n",
            "Number of Epoch = 70 - Average Loss:= 0.15646916015625 Validation Accuracy:= 66.43999814987183 Training Accuracy:= 67.23799705505371\n",
            "Number of Epoch = 75 - Average Loss:= 0.156675693359375 Validation Accuracy:= 66.50999784469604 Training Accuracy:= 67.35799908638\n",
            "Number of Epoch = 80 - Average Loss:= 0.157903095703125 Validation Accuracy:= 66.42000079154968 Training Accuracy:= 67.31399893760681\n",
            "Number of Epoch = 85 - Average Loss:= 0.1573398046875 Validation Accuracy:= 66.38000011444092 Training Accuracy:= 67.14400053024292\n",
            "Number of Epoch = 90 - Average Loss:= 0.155226044921875 Validation Accuracy:= 66.62999987602234 Training Accuracy:= 67.58999824523926\n",
            "Number of Epoch = 95 - Average Loss:= 0.15546283203125 Validation Accuracy:= 66.74000024795532 Training Accuracy:= 67.71399974822998\n",
            "Number of Epoch = 100 - Average Loss:= 0.15534333984375 Validation Accuracy:= 66.93000197410583 Training Accuracy:= 67.90599822998047\n",
            "\n",
            "Total time taken (in seconds): 2763.59\n",
            "Inference Loss: 0.0834\n",
            "Inference Accuracy: 67.0084\n",
            "For Seed 876 :\n",
            "Number of Epoch = 5 - Average Loss:= 0.18859423828125 Validation Accuracy:= 55.26999831199646 Training Accuracy:= 54.78600263595581\n",
            "Number of Epoch = 10 - Average Loss:= 0.17457103515625 Validation Accuracy:= 63.92999887466431 Training Accuracy:= 63.24999928474426\n",
            "Number of Epoch = 15 - Average Loss:= 0.16968271484375 Validation Accuracy:= 69.0999984741211 Training Accuracy:= 69.01000142097473\n",
            "Number of Epoch = 20 - Average Loss:= 0.16207861328125 Validation Accuracy:= 72.97000288963318 Training Accuracy:= 72.51399755477905\n",
            "Number of Epoch = 25 - Average Loss:= 0.16115326171875 Validation Accuracy:= 73.91999959945679 Training Accuracy:= 73.5480010509491\n",
            "Number of Epoch = 30 - Average Loss:= 0.158638232421875 Validation Accuracy:= 74.4599997997284 Training Accuracy:= 74.41400289535522\n",
            "Number of Epoch = 35 - Average Loss:= 0.1567605078125 Validation Accuracy:= 74.87000226974487 Training Accuracy:= 74.8199999332428\n",
            "Number of Epoch = 40 - Average Loss:= 0.15446232421875 Validation Accuracy:= 75.05000233650208 Training Accuracy:= 75.07799863815308\n",
            "Number of Epoch = 45 - Average Loss:= 0.15515548828125 Validation Accuracy:= 75.08000135421753 Training Accuracy:= 75.33400058746338\n",
            "Number of Epoch = 50 - Average Loss:= 0.153966142578125 Validation Accuracy:= 75.19999742507935 Training Accuracy:= 75.52199959754944\n",
            "Number of Epoch = 55 - Average Loss:= 0.15282146484375 Validation Accuracy:= 75.5400002002716 Training Accuracy:= 75.90199708938599\n",
            "Number of Epoch = 60 - Average Loss:= 0.154124228515625 Validation Accuracy:= 75.72000026702881 Training Accuracy:= 76.24599933624268\n",
            "Number of Epoch = 65 - Average Loss:= 0.152731982421875 Validation Accuracy:= 75.78999996185303 Training Accuracy:= 76.35800242424011\n",
            "Number of Epoch = 70 - Average Loss:= 0.15297251953125 Validation Accuracy:= 76.010000705719 Training Accuracy:= 76.52199864387512\n",
            "Number of Epoch = 75 - Average Loss:= 0.15117251953125 Validation Accuracy:= 76.03999972343445 Training Accuracy:= 76.64999961853027\n",
            "Number of Epoch = 80 - Average Loss:= 0.15151498046875 Validation Accuracy:= 75.62000155448914 Training Accuracy:= 76.23599767684937\n",
            "Number of Epoch = 85 - Average Loss:= 0.15243642578125 Validation Accuracy:= 76.05999708175659 Training Accuracy:= 76.74800157546997\n",
            "Number of Epoch = 90 - Average Loss:= 0.1516344140625 Validation Accuracy:= 76.2499988079071 Training Accuracy:= 76.88999772071838\n",
            "Number of Epoch = 95 - Average Loss:= 0.15213462890625 Validation Accuracy:= 76.13000273704529 Training Accuracy:= 76.75600051879883\n",
            "Number of Epoch = 100 - Average Loss:= 0.1537119140625 Validation Accuracy:= 76.26000046730042 Training Accuracy:= 76.99000239372253\n",
            "\n",
            "Total time taken (in seconds): 3039.16\n",
            "Inference Loss: 0.0608\n",
            "Inference Accuracy: 75.9156\n",
            "For Seed 6666 :\n",
            "Number of Epoch = 5 - Average Loss:= 0.20049951171875 Validation Accuracy:= 43.11999976634979 Training Accuracy:= 42.01599955558777\n",
            "Number of Epoch = 10 - Average Loss:= 0.19440806640625 Validation Accuracy:= 45.260000228881836 Training Accuracy:= 44.36799883842468\n",
            "Number of Epoch = 15 - Average Loss:= 0.191643828125 Validation Accuracy:= 45.660001039505005 Training Accuracy:= 44.86599862575531\n",
            "Number of Epoch = 20 - Average Loss:= 0.190141953125 Validation Accuracy:= 45.95000147819519 Training Accuracy:= 45.32000124454498\n",
            "Number of Epoch = 25 - Average Loss:= 0.18370373046875 Validation Accuracy:= 53.35000157356262 Training Accuracy:= 52.17199921607971\n",
            "Number of Epoch = 30 - Average Loss:= 0.1803726953125 Validation Accuracy:= 54.49000000953674 Training Accuracy:= 53.512001037597656\n",
            "Number of Epoch = 35 - Average Loss:= 0.17840345703125 Validation Accuracy:= 54.919999837875366 Training Accuracy:= 54.05799746513367\n",
            "Number of Epoch = 40 - Average Loss:= 0.1775109375 Validation Accuracy:= 54.80999946594238 Training Accuracy:= 54.05399799346924\n",
            "Number of Epoch = 45 - Average Loss:= 0.17662083984375 Validation Accuracy:= 55.320000648498535 Training Accuracy:= 54.80800271034241\n",
            "Number of Epoch = 50 - Average Loss:= 0.17528630859375 Validation Accuracy:= 55.58000206947327 Training Accuracy:= 55.11400103569031\n",
            "Number of Epoch = 55 - Average Loss:= 0.17383359375 Validation Accuracy:= 55.69000244140625 Training Accuracy:= 55.10200262069702\n",
            "Number of Epoch = 60 - Average Loss:= 0.17426125 Validation Accuracy:= 55.52999973297119 Training Accuracy:= 55.27200102806091\n",
            "Number of Epoch = 65 - Average Loss:= 0.173537265625 Validation Accuracy:= 55.83000183105469 Training Accuracy:= 55.56600093841553\n",
            "Number of Epoch = 70 - Average Loss:= 0.174120078125 Validation Accuracy:= 55.82000017166138 Training Accuracy:= 55.545997619628906\n",
            "Number of Epoch = 75 - Average Loss:= 0.1727254296875 Validation Accuracy:= 56.01999759674072 Training Accuracy:= 55.83199858665466\n",
            "Number of Epoch = 80 - Average Loss:= 0.17183009765625 Validation Accuracy:= 56.05999827384949 Training Accuracy:= 55.96399903297424\n",
            "Number of Epoch = 85 - Average Loss:= 0.17236923828125 Validation Accuracy:= 55.80000281333923 Training Accuracy:= 55.75000047683716\n",
            "Number of Epoch = 90 - Average Loss:= 0.17261955078125 Validation Accuracy:= 56.08999729156494 Training Accuracy:= 56.022000312805176\n",
            "Number of Epoch = 95 - Average Loss:= 0.17180392578125 Validation Accuracy:= 55.970001220703125 Training Accuracy:= 55.893999338150024\n",
            "Number of Epoch = 100 - Average Loss:= 0.17071943359375 Validation Accuracy:= 56.25 Training Accuracy:= 56.129997968673706\n",
            "\n",
            "Total time taken (in seconds): 2534.16\n",
            "Inference Loss: 0.1121\n",
            "Inference Accuracy: 55.6429\n",
            "For Seed 7777 :\n",
            "Number of Epoch = 5 - Average Loss:= 0.212711015625 Validation Accuracy:= 25.589999556541443 Training Accuracy:= 25.775998830795288\n",
            "Number of Epoch = 10 - Average Loss:= 0.20613171875 Validation Accuracy:= 34.33000147342682 Training Accuracy:= 33.61000120639801\n",
            "Number of Epoch = 15 - Average Loss:= 0.19945650390625 Validation Accuracy:= 35.589998960494995 Training Accuracy:= 35.43800115585327\n",
            "Number of Epoch = 20 - Average Loss:= 0.19126521484375 Validation Accuracy:= 45.87000012397766 Training Accuracy:= 46.27600014209747\n",
            "Number of Epoch = 25 - Average Loss:= 0.1835360546875 Validation Accuracy:= 54.100000858306885 Training Accuracy:= 54.86400127410889\n",
            "Number of Epoch = 30 - Average Loss:= 0.177699765625 Validation Accuracy:= 55.41999936103821 Training Accuracy:= 56.42600059509277\n",
            "Number of Epoch = 35 - Average Loss:= 0.177149140625 Validation Accuracy:= 55.01999855041504 Training Accuracy:= 55.92600107192993\n",
            "Number of Epoch = 40 - Average Loss:= 0.17499056640625 Validation Accuracy:= 54.76999878883362 Training Accuracy:= 55.970001220703125\n",
            "Number of Epoch = 45 - Average Loss:= 0.17304970703125 Validation Accuracy:= 55.46000003814697 Training Accuracy:= 56.606000661849976\n",
            "Number of Epoch = 50 - Average Loss:= 0.17287412109375 Validation Accuracy:= 61.29000186920166 Training Accuracy:= 62.04400062561035\n",
            "Number of Epoch = 55 - Average Loss:= 0.1699516015625 Validation Accuracy:= 64.06999826431274 Training Accuracy:= 64.51600193977356\n",
            "Number of Epoch = 60 - Average Loss:= 0.168413671875 Validation Accuracy:= 64.55000042915344 Training Accuracy:= 65.06400108337402\n",
            "Number of Epoch = 65 - Average Loss:= 0.1666944140625 Validation Accuracy:= 64.82999920845032 Training Accuracy:= 65.57000279426575\n",
            "Number of Epoch = 70 - Average Loss:= 0.1661966796875 Validation Accuracy:= 65.03000259399414 Training Accuracy:= 65.66399931907654\n",
            "Number of Epoch = 75 - Average Loss:= 0.163998984375 Validation Accuracy:= 64.78000283241272 Training Accuracy:= 65.85599780082703\n",
            "Number of Epoch = 80 - Average Loss:= 0.16344853515625 Validation Accuracy:= 65.10999798774719 Training Accuracy:= 66.0640001296997\n",
            "Number of Epoch = 85 - Average Loss:= 0.164710859375 Validation Accuracy:= 65.49000144004822 Training Accuracy:= 66.44999980926514\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-19-d29466b2ee84>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m       \u001b[0macc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_total_gpu\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mmlp_on_default\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m       \u001b[0mlt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlt\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mmlp_on_default\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m       \u001b[0mmlp_on_default\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0mAccpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmlp_on_default\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-17-96cbaab40dbc>\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, X_train, y_train)\u001b[0m\n\u001b[1;32m     64\u001b[0m       \u001b[0mcurrent_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredicted\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0mgrads\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcurrent_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvariables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_gradients\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrads\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvariables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m     '''\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/optimizer_v2.py\u001b[0m in \u001b[0;36mapply_gradients\u001b[0;34m(self, grads_and_vars, name, experimental_aggregate_gradients)\u001b[0m\n\u001b[1;32m    671\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0moptimizer_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrategy_supports_no_merge_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    672\u001b[0m         return self._distributed_apply(strategy, grads_and_vars, name,\n\u001b[0;32m--> 673\u001b[0;31m                                        apply_state)\n\u001b[0m\u001b[1;32m    674\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    675\u001b[0m         return tf.distribute.get_replica_context().merge_call(\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/optimizer_v2.py\u001b[0m in \u001b[0;36m_distributed_apply\u001b[0;34m(self, distribution, grads_and_vars, name, apply_state)\u001b[0m\n\u001b[1;32m    740\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miterations\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0massign_add\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mread_value\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    741\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 742\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miterations\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0massign_add\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    743\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    744\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mget_gradients\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/resource_variable_ops.py\u001b[0m in \u001b[0;36massign_add\u001b[0;34m(self, delta, use_locking, name, read_value)\u001b[0m\n\u001b[1;32m    868\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    869\u001b[0m           \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdelta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 870\u001b[0;31m           name=name)\n\u001b[0m\u001b[1;32m    871\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mread_value\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    872\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lazy_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0massign_add_op\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/gen_resource_variable_ops.py\u001b[0m in \u001b[0;36massign_add_variable_op\u001b[0;34m(resource, value, name)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m       _result = pywrap_tfe.TFE_Py_FastPathExecute(\n\u001b[0;32m---> 44\u001b[0;31m         _ctx, \"AssignAddVariableOp\", name, resource, value)\n\u001b[0m\u001b[1;32m     45\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "seed = [56,876,6666,7777,3452,5]\n",
        "\n",
        "for x in seed:\n",
        "  print('For Seed',x,':')\n",
        "\n",
        "  #Default mode\n",
        "  mlp_on_default = MLP(size_input, size_hidden1, size_hidden2, size_output, device='tpu')\n",
        "  #hidden2 = MLP(size_hidden2, size_hidden2, size_output, device='gpu')\n",
        "  time_start = time.time()\n",
        "\n",
        "  for epoch in range(NUM_EPOCHS):\n",
        "    loss_total_gpu = tf.zeros([1,1], dtype=tf.float32)\n",
        "    lt = 0\n",
        "    acc = 0\n",
        "    train_ds = tf.data.Dataset.from_tensor_slices((X_train, y_train)).shuffle(25, seed=epoch*(x)).batch(64)\n",
        "    accuracies = []\n",
        "    for inputs, outputs in train_ds:\n",
        "      preds = mlp_on_default.forward(inputs,True)\n",
        "      loss_total_gpu = loss_total_gpu + mlp_on_default.loss(preds, outputs)\n",
        "      acc = loss_total_gpu + mlp_on_default.loss(preds, outputs)\n",
        "      lt = lt + mlp_on_default.loss(preds, outputs)\n",
        "      mlp_on_default.backward(inputs, outputs)\n",
        "      \n",
        "    Accpred = mlp_on_default.forward(X_train)\n",
        "    cat_acc = tf.metrics.CategoricalAccuracy()\n",
        "    accuracies.append(cat_acc(Accpred, y_train))\n",
        "    ValAccuracies = []\n",
        "    Accpred = mlp_on_default.forward(X_val)\n",
        "    cat_acc = tf.metrics.CategoricalAccuracy()\n",
        "    ValAccuracies.append(cat_acc(Accpred, y_val))\n",
        "    if((epoch+1)%5 == 0):\n",
        "      print('Number of Epoch = {} - Average Loss:= {} Validation Accuracy:= {} Training Accuracy:= {}'.format(epoch + 1, np.sum(loss_total_gpu) / X_train.shape[0],np.mean(ValAccuracies)*100,np.mean(accuracies)*100))\n",
        "\n",
        "  time_taken = time.time() - time_start\n",
        "  print('\\nTotal time taken (in seconds): {:.2f}'.format(time_taken))\n",
        "\n",
        "  #inference\n",
        "  test_loss_total = tf.Variable(0, dtype=tf.float32)\n",
        "  #test_loss_total = 0.0\n",
        "  test_ds = tf.data.Dataset.from_tensor_slices((X_test, y_test)).shuffle(25, seed=epoch*(x)).batch(64)\n",
        "  cat_acc = tf.metrics.CategoricalAccuracy()\n",
        "  accuracies = []\n",
        "\n",
        "  for inputs, outputs in test_ds:\n",
        "    preds = mlp_on_default.forward(inputs)\n",
        "    test_loss_total = test_loss_total + mlp_on_default.loss(preds, outputs)\n",
        "    cat_acc = tf.metrics.CategoricalAccuracy()  \n",
        "    accuracies.append(cat_acc(preds, outputs))\n",
        "\n",
        "\n",
        "  print('Inference Loss: {:.4f}'.format(np.sum(test_loss_total.numpy()) / X_test.shape[0]))\n",
        "  print('Inference Accuracy: {:.4f}'.format(np.mean(accuracies)*100))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PXe-2MENCOjq"
      },
      "source": [
        "## One Step Inference"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "MNIST_With_Dropout.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}