{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AnkurMali/IST597_Spring_2022/blob/main/IST597_MLP_collab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "71kdFp0QgF4K"
      },
      "source": [
        "# IST597:- Multi-Layer Perceptron"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C2yHcl5xgPV1"
      },
      "source": [
        "## Load the libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2DPwxLR2gSLC"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import time\n",
        "import tensorflow as tf\n",
        "np.random.seed(1234)\n",
        "tf.random.set_seed(1234)\n",
        "from tensorflow.keras.datasets import fashion_mnist\n",
        "from sklearn.preprocessing import LabelBinarizer\n",
        "from sklearn.metrics import roc_auc_score, accuracy_score\n",
        "from sklearn.metrics import precision_score, accuracy_score\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wV-3kEaggcO8",
        "outputId": "9d1fd7a2-8d32-46b0-afb7-6e7d134a21b6"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[]"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tf.config.list_physical_devices('GPU')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iw78jw6pDqSM"
      },
      "source": [
        "#Get number of Gpu's and id's in the system or else you can also use Nvidia-smi in command prompt."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Dk_S2TMg_6_"
      },
      "source": [
        "## Generate random data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "40XlFnwho7D8"
      },
      "outputs": [],
      "source": [
        "size_input = 784\n",
        "size_hidden1 = 256\n",
        "size_hidden2 = 128\n",
        "size_output = 10\n",
        "\n",
        "number_of_train_examples = 6000\n",
        "number_of_test_examples = 1000"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 693
        },
        "id": "qm23CzRihaW0",
        "outputId": "4d11d4f2-21e6-4832-b973-770821abf485"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz\n",
            "32768/29515 [=================================] - 0s 0us/step\n",
            "40960/29515 [=========================================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz\n",
            "26427392/26421880 [==============================] - 0s 0us/step\n",
            "26435584/26421880 [==============================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz\n",
            "16384/5148 [===============================================================================================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz\n",
            "4423680/4422102 [==============================] - 0s 0us/step\n",
            "4431872/4422102 [==============================] - 0s 0us/step\n",
            "(50000, 28, 28) (50000,)\n",
            "Train dimension:\n",
            "(50000, 784)\n",
            "Test dimension:\n",
            "(10000, 784)\n",
            "Val dimension:\n",
            "(10000, 784)\n",
            "Train labels dimension:\n",
            "(50000, 10)\n",
            "Test labels dimension:\n",
            "(10000, 10)\n",
            "Test labels dimension:\n",
            "(10000, 10)\n"
          ]
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "'''\n",
        "X_train = np.random.randn(number_of_train_examples , size_input)\n",
        "y_train = np.random.randn(number_of_train_examples)\n",
        "X_test = np.random.randn(number_of_test_examples, size_input)\n",
        "y_test = np.random.randn(number_of_test_examples)\n",
        "'''\n",
        "\n",
        "\n",
        "def load_dataset(flatten=False):\n",
        "    (X_train, y_train), (X_test, y_test) = fashion_mnist.load_data()\n",
        "    # normalize x\n",
        "    X_train = X_train.astype(float)/255.0\n",
        "    X_test = X_test.astype(float)/255.0\n",
        "    # we reserve the last 10000 training examples for validation\n",
        "    X_train, X_val = X_train[:-10000], X_train[-10000:]\n",
        "    y_train, y_val = y_train[:-10000], y_train[-10000:]\n",
        "    if flatten:\n",
        "        X_train = X_train.reshape([X_train.shape[0], -1])\n",
        "        X_val = X_val.reshape([X_val.shape[0], -1])\n",
        "        X_test = X_test.reshape([X_test.shape[0], -1])\n",
        "    return X_train, y_train, X_val, y_val, X_test, y_test\n",
        "\n",
        "X_train, y_train, X_val, y_val, X_test, y_test = load_dataset()\n",
        "## Printing dimensions\n",
        "print(X_train.shape, y_train.shape)\n",
        "## Visualizing the first digit\n",
        "## Changing dimension of input images from N*28*28 to  N*784\n",
        "X_train = X_train.reshape((X_train.shape[0],X_train.shape[1]*X_train.shape[2]))\n",
        "X_test = X_test.reshape((X_test.shape[0],X_test.shape[1]*X_test.shape[2]))\n",
        "X_val = X_val.reshape((X_val.shape[0],X_val.shape[1]*X_val.shape[2]))\n",
        "\n",
        "print('Train dimension:');print(X_train.shape)\n",
        "print('Test dimension:');print(X_test.shape)\n",
        "print('Val dimension:');print(X_val.shape)\n",
        "\n",
        "## Changing labels to one-hot encoded vector\n",
        "lb = LabelBinarizer()\n",
        "y_train = lb.fit_transform(y_train)\n",
        "y_test = lb.transform(y_test)\n",
        "y_val = lb.transform(y_val)\n",
        "\n",
        "print('Train labels dimension:');print(y_train.shape)\n",
        "print('Test labels dimension:');print(y_test.shape)\n",
        "print('Test labels dimension:');print(y_val.shape)\n",
        "\n",
        "\n",
        "def Accuracy(yTrue,yPred):\n",
        "    return tf.keras.metrics.sparse_categorical_accuracy(yTrue, yPred)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aigqKFFF5BM2"
      },
      "outputs": [],
      "source": [
        "# Split dataset into batches\n",
        "train_ds = tf.data.Dataset.from_tensor_slices((X_train, y_train)).batch(128)\n",
        "test_ds = tf.data.Dataset.from_tensor_slices((X_test, y_test)).batch(128)\n",
        "\n",
        "\n",
        "   "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Eb4hOoVbnzSJ"
      },
      "source": [
        "## Build MLP using Eager Execution"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ht9_qpYipgHw"
      },
      "outputs": [],
      "source": [
        "# Define class to build mlp model\n",
        "class MLP(object):\n",
        "  def __init__(self, size_input, size_hidden1,size_hidden2,size_output, device=None):\n",
        "    \"\"\"\n",
        "    size_input: int, size of input layer\n",
        "    size_hidden: int, size of hidden layer\n",
        "    size_output: int, size of output layer\n",
        "    device: str or None, either 'cpu' or 'gpu' or None. If None, the device to be used will be decided automatically during Eager Execution\n",
        "    \"\"\"\n",
        "    self.size_input, self.size_hidden1,self.size_hidden2, self.size_output, self.device =\\\n",
        "    size_input, size_hidden1, size_hidden2, size_output, device\n",
        "    \n",
        "    # Initialize weights between input layer and hidden layer\n",
        "    self.W1 = tf.Variable(tf.random.normal([self.size_input, self.size_hidden1]))\n",
        "    # Initialize biases for hidden layer\n",
        "    self.b1 = tf.Variable(tf.random.normal([1, self.size_hidden1]))\n",
        "     # Initialize weights between hidden layer1 and hidden layer2 layer\n",
        "    self.W2 = tf.Variable(tf.random.normal([self.size_hidden1, self.size_hidden2]))\n",
        "    # Initialize biases for hidden layer1 and hidden layer2 layer\n",
        "    self.b2 = tf.Variable(tf.random.normal([1, self.size_hidden2]))\n",
        "    # Initialize weights between hidden layer1 and hidden layer2 layer\n",
        "    self.W3 = tf.Variable(tf.random.normal([self.size_hidden2, self.size_output]))\n",
        "    # Initialize biases for hidden layer1 and hidden layer2 layer\n",
        "    self.b3 = tf.Variable(tf.random.normal([1, self.size_output]))\n",
        "    \n",
        "    # Define variables to be updated during backpropagation\n",
        "    self.variables = [self.W1, self.W2, self.b1, self.b2,self.W3,self.b3]\n",
        "    \n",
        "  def forward(self, X):\n",
        "    \"\"\"\n",
        "    forward pass\n",
        "    X: Tensor, inputs\n",
        "    \"\"\"\n",
        "    if self.device is not None:\n",
        "      with tf.device('gpu:0' if self.device=='gpu' else 'cpu'):\n",
        "        self.y = self.compute_output(X)\n",
        "    else:\n",
        "      self.y = self.compute_output(X)\n",
        "      \n",
        "    return self.y\n",
        "  \n",
        "  def loss(self, y_pred, y_true):\n",
        "    '''\n",
        "    y_pred - Tensor of shape (batch_size, size_output)\n",
        "    y_true - Tensor of shape (batch_size, size_output)\n",
        "    '''\n",
        "    y_true_tf = tf.cast(tf.reshape(y_true, (-1, self.size_output)), dtype=tf.float32)\n",
        "    y_pred_tf = tf.cast(y_pred, dtype=tf.float32)\n",
        "    cce = tf.keras.losses.CategoricalCrossentropy()\n",
        "    #print(y_true_tf.shape,y_pred_tf.shape)\n",
        "    #print('succ',cce(y_true_tf, y_pred_tf).numpy())\n",
        "    loss = cce(y_true_tf, y_pred_tf)\n",
        "    beta = 0.01\n",
        "    # Loss function with L1 Regularization with beta=0.01\n",
        "\n",
        "    regularizers = (tf.keras.regularizers.l1(0.5)(self.W1)) + (tf.keras.regularizers.l1(0.5)(self.W2)) + (tf.keras.regularizers.l1(0.5)(self.W3))\n",
        "    loss = tf.reduce_mean(loss + beta * regularizers)\n",
        "    return loss\n",
        "    #return tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=y_pred_tf, labels=y_true_tf))\n",
        "    #return tf.losses.mean_squared_error(y_true_tf, y_pred_tf)\n",
        "  \n",
        "  def backward(self, X_train, y_train):\n",
        "    \"\"\"\n",
        "    backward pass\n",
        "    \"\"\"    \n",
        "    \n",
        "    optimizer = tf.keras.optimizers.SGD(learning_rate=1e-2)\n",
        "    \n",
        "    with tf.GradientTape() as tape:\n",
        "      predicted = self.forward(X_train)\n",
        "      current_loss = self.loss(predicted, y_train)\n",
        "    grads = tape.gradient(current_loss, self.variables)\n",
        "    optimizer.apply_gradients(zip(grads, self.variables))\n",
        "    \n",
        "    '''\n",
        "\n",
        "    with tf.GradientTape(persistent=True) as tape3:\n",
        "      with tf.GradientTape(persistent=True) as tape2:\n",
        "        with tf.GradientTape(persistent=True) as tape1:\n",
        "          predicted = self.forward(X_train)\n",
        "          current_loss = self.loss(predicted, y_train)\n",
        "        grad1 = tape1.gradient(current_loss, [self.W3,self.b3])\n",
        "        optimizer.apply_gradients(zip(grad1, [self.W3,self.b3]))\n",
        "      grad2 = tape2.gradient(grad1,[self.W2,self.b2])\n",
        "      optimizer.apply_gradients(zip(grad2, [self.W2,self.b2]))\n",
        "    grad3 = tape3.gradient(grad2, [self.W1,self.b1])\n",
        "    optimizer.apply_gradients(zip(grad3, [self.W1,self.b1]))\n",
        "    return grad3\n",
        "\n",
        "    '''\n",
        "        \n",
        "  def compute_output(self, X):\n",
        "    \"\"\"\n",
        "    Custom method to obtain output tensor during forward pass\n",
        "    \"\"\"\n",
        "    # Cast X to float32\n",
        "    X_tf = tf.cast(X, dtype=tf.float32)\n",
        "    #Remember to normalize your dataset before moving forward\n",
        "    # Compute values in hidden layer1\n",
        "    what = tf.matmul(X_tf, self.W1) + self.b1\n",
        "    hhat = tf.nn.relu(what)\n",
        "    # Compute values in hidden layer2\n",
        "    what2 = tf.matmul(hhat, self.W2) + self.b2\n",
        "    hhat2 = tf.nn.relu(what2)\n",
        "    # Compute output\n",
        "    output = tf.matmul(hhat2, self.W3) + self.b3\n",
        "    #Now consider two things , First look at inbuild loss functions if they work with softmax or not and then change this\n",
        "    #Second add tf.Softmax(output) and then return this variable\n",
        "    return tf.nn.softmax(output)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cUDFOuNk618X"
      },
      "source": [
        "## Train Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FZPVUu0YDa-_"
      },
      "outputs": [],
      "source": [
        "# Set number of epochs\n",
        "NUM_EPOCHS = 50"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "xI4lsqPhB6Xi",
        "outputId": "34a47378-3132-4663-f0d2-1c71b46898ce"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "For Seed 56 :\n",
            "Number of Epoch = 5 - Average Loss:= 11.79184625 Validation Accuracy:= 45.159998536109924 Training Accuracy:= 45.56800127029419\n",
            "Number of Epoch = 10 - Average Loss:= 8.973001875 Validation Accuracy:= 47.69999980926514 Training Accuracy:= 47.835999727249146\n",
            "Number of Epoch = 15 - Average Loss:= 6.678713125 Validation Accuracy:= 47.58000075817108 Training Accuracy:= 47.9640007019043\n",
            "Number of Epoch = 20 - Average Loss:= 4.859801875 Validation Accuracy:= 47.79999852180481 Training Accuracy:= 48.30999970436096\n",
            "Number of Epoch = 25 - Average Loss:= 3.43901875 Validation Accuracy:= 53.850001096725464 Training Accuracy:= 55.08400201797485\n",
            "Number of Epoch = 30 - Average Loss:= 2.31891828125 Validation Accuracy:= 72.13000059127808 Training Accuracy:= 73.25599789619446\n",
            "Number of Epoch = 35 - Average Loss:= 1.5191496875 Validation Accuracy:= 78.07000279426575 Training Accuracy:= 78.79199981689453\n",
            "Number of Epoch = 40 - Average Loss:= 0.98423828125 Validation Accuracy:= 76.75999999046326 Training Accuracy:= 77.95400023460388\n",
            "Number of Epoch = 45 - Average Loss:= 0.6227921875 Validation Accuracy:= 75.59000253677368 Training Accuracy:= 76.66199803352356\n",
            "Number of Epoch = 50 - Average Loss:= 0.3872765625 Validation Accuracy:= 75.84999799728394 Training Accuracy:= 76.774001121521\n",
            "\n",
            "Total time taken (in seconds): 1643.66\n",
            "Inference Loss: 0.3716\n",
            "Inference Accuracy: 75.4479\n",
            "For Seed 876 :\n",
            "Number of Epoch = 5 - Average Loss:= 11.86199875 Validation Accuracy:= 42.03999936580658 Training Accuracy:= 42.862001061439514\n",
            "Number of Epoch = 10 - Average Loss:= 8.986261875 Validation Accuracy:= 58.27000141143799 Training Accuracy:= 59.38199758529663\n",
            "Number of Epoch = 15 - Average Loss:= 6.685076875 Validation Accuracy:= 60.53000092506409 Training Accuracy:= 61.813998222351074\n",
            "Number of Epoch = 20 - Average Loss:= 4.8554925 Validation Accuracy:= 61.51999831199646 Training Accuracy:= 62.90599703788757\n",
            "Number of Epoch = 25 - Average Loss:= 3.4405790625 Validation Accuracy:= 61.34999990463257 Training Accuracy:= 63.02800178527832\n",
            "Number of Epoch = 30 - Average Loss:= 2.30749546875 Validation Accuracy:= 71.56000137329102 Training Accuracy:= 73.22800159454346\n",
            "Number of Epoch = 35 - Average Loss:= 1.53358875 Validation Accuracy:= 77.18999981880188 Training Accuracy:= 78.8860023021698\n",
            "Number of Epoch = 40 - Average Loss:= 0.995579453125 Validation Accuracy:= 76.87000036239624 Training Accuracy:= 78.46599817276001\n",
            "Number of Epoch = 45 - Average Loss:= 0.6311675 Validation Accuracy:= 76.98000073432922 Training Accuracy:= 77.85599827766418\n",
            "Number of Epoch = 50 - Average Loss:= 0.391236015625 Validation Accuracy:= 78.04999947547913 Training Accuracy:= 78.86599898338318\n",
            "\n",
            "Total time taken (in seconds): 1713.61\n",
            "Inference Loss: 0.3741\n",
            "Inference Accuracy: 77.6174\n",
            "For Seed 6666 :\n",
            "Number of Epoch = 5 - Average Loss:= 11.81741875 Validation Accuracy:= 46.149998903274536 Training Accuracy:= 46.682000160217285\n",
            "Number of Epoch = 10 - Average Loss:= 8.998211875 Validation Accuracy:= 46.91999852657318 Training Accuracy:= 47.33000099658966\n",
            "Number of Epoch = 15 - Average Loss:= 6.701756875 Validation Accuracy:= 47.15999960899353 Training Accuracy:= 47.73800075054169\n",
            "Number of Epoch = 20 - Average Loss:= 4.8842846875 Validation Accuracy:= 47.33999967575073 Training Accuracy:= 47.87000119686127\n",
            "Number of Epoch = 25 - Average Loss:= 3.4638178125 Validation Accuracy:= 53.46999764442444 Training Accuracy:= 53.710001707077026\n",
            "Number of Epoch = 30 - Average Loss:= 2.40739203125 Validation Accuracy:= 54.79999780654907 Training Accuracy:= 55.15999794006348\n",
            "Number of Epoch = 35 - Average Loss:= 1.5444884375 Validation Accuracy:= 75.40000081062317 Training Accuracy:= 76.59000158309937\n",
            "Number of Epoch = 40 - Average Loss:= 1.003228203125 Validation Accuracy:= 76.8999993801117 Training Accuracy:= 78.26600074768066\n",
            "Number of Epoch = 45 - Average Loss:= 0.6370514453125 Validation Accuracy:= 78.24000120162964 Training Accuracy:= 79.29999828338623\n",
            "Number of Epoch = 50 - Average Loss:= 0.3963263671875 Validation Accuracy:= 76.2399971485138 Training Accuracy:= 77.28800177574158\n",
            "\n",
            "Total time taken (in seconds): 1732.26\n",
            "Inference Loss: 0.3798\n",
            "Inference Accuracy: 76.4928\n",
            "For Seed 7777 :\n",
            "Number of Epoch = 5 - Average Loss:= 11.85463125 Validation Accuracy:= 19.529999792575836 Training Accuracy:= 19.575999677181244\n"
          ]
        }
      ],
      "source": [
        "seed = [56,876,6666,7777,3452,5]\n",
        "\n",
        "for x in seed:\n",
        "  print('For Seed',x,':')\n",
        "\n",
        "  #Default mode\n",
        "  mlp_on_default = MLP(size_input, size_hidden1, size_hidden2, size_output, device='tpu')\n",
        "  #hidden2 = MLP(size_hidden2, size_hidden2, size_output, device='gpu')\n",
        "  time_start = time.time()\n",
        "\n",
        "  for epoch in range(NUM_EPOCHS):\n",
        "    loss_total_gpu = tf.zeros([1,1], dtype=tf.float32)\n",
        "    lt = 0\n",
        "    acc = 0\n",
        "    train_ds = tf.data.Dataset.from_tensor_slices((X_train, y_train)).shuffle(25, seed=epoch*(x)).batch(64)\n",
        "    accuracies = []\n",
        "    for inputs, outputs in train_ds:\n",
        "      preds = mlp_on_default.forward(inputs)\n",
        "      loss_total_gpu = loss_total_gpu + mlp_on_default.loss(preds, outputs)\n",
        "      acc = loss_total_gpu + mlp_on_default.loss(preds, outputs)\n",
        "      lt = lt + mlp_on_default.loss(preds, outputs)\n",
        "      mlp_on_default.backward(inputs, outputs)\n",
        "      \n",
        "    Accpred = mlp_on_default.forward(X_train)\n",
        "    cat_acc = tf.metrics.CategoricalAccuracy()\n",
        "    accuracies.append(cat_acc(Accpred, y_train))\n",
        "    ValAccuracies = []\n",
        "    Accpred = mlp_on_default.forward(X_val)\n",
        "    cat_acc = tf.metrics.CategoricalAccuracy()\n",
        "    ValAccuracies.append(cat_acc(Accpred, y_val))\n",
        "    if((epoch+1)%5 == 0):\n",
        "      print('Number of Epoch = {} - Average Loss:= {} Validation Accuracy:= {} Training Accuracy:= {}'.format(epoch + 1, np.sum(loss_total_gpu) / X_train.shape[0],np.mean(ValAccuracies)*100,np.mean(accuracies)*100))\n",
        "\n",
        "  time_taken = time.time() - time_start\n",
        "  print('\\nTotal time taken (in seconds): {:.2f}'.format(time_taken))\n",
        "\n",
        "  #inference\n",
        "  test_loss_total = tf.Variable(0, dtype=tf.float32)\n",
        "  #test_loss_total = 0.0\n",
        "  test_ds = tf.data.Dataset.from_tensor_slices((X_test, y_test)).shuffle(25, seed=epoch*(x)).batch(64)\n",
        "  cat_acc = tf.metrics.CategoricalAccuracy()\n",
        "  accuracies = []\n",
        "\n",
        "  for inputs, outputs in test_ds:\n",
        "    preds = mlp_on_default.forward(inputs)\n",
        "    test_loss_total = test_loss_total + mlp_on_default.loss(preds, outputs)\n",
        "    cat_acc = tf.metrics.CategoricalAccuracy()  \n",
        "    accuracies.append(cat_acc(preds, outputs))\n",
        "\n",
        "\n",
        "  print('Inference Loss: {:.4f}'.format(np.sum(test_loss_total.numpy()) / X_test.shape[0]))\n",
        "  print('Inference Accuracy: {:.4f}'.format(np.mean(accuracies)*100))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PXe-2MENCOjq"
      },
      "source": [
        "## One Step Inference"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "FashionMNIST_With_L1Regularization.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}